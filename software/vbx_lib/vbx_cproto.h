//VBXCOPYRIGHTTAG
#ifndef __VBX_CPROTO_H
#define __VBX_CPROTO_H
#define ENUM_PARAM vbx_enum_t *v_enum __attribute__((unused))
#define VVWWWSS VVWWWSSS
#define VVWWWS  VVWWWSSS
#define VVWWWUU VVWWWUUU
#define VVWWWU  VVWWWUUU
#define VVWWW   VVWWWSSS
#define VVWWSSS VVWWWSSS
#define VVWWSS  VVWWWSSS
#define VVWWS   VVWWWSSS
#define VVWWUUU VVWWWUUU
#define VVWWUU  VVWWWUUU
#define VVWWU   VVWWWUUU
#define VVWW    VVWWWSSS
#define VVWSSS  VVWWWSSS
#define VVWSS   VVWWWSSS
#define VVWS    VVWWWSSS
#define VVWUUU  VVWWWUUU
#define VVWUU   VVWWWUUU
#define VVWU    VVWWWUUU
#define VVW     VVWWWSSS
#define VVHHHSS VVHHHSSS
#define VVHHHS  VVHHHSSS
#define VVHHHUU VVHHHUUU
#define VVHHHU  VVHHHUUU
#define VVHHH   VVHHHSSS
#define VVHHSSS VVHHHSSS
#define VVHHSS  VVHHHSSS
#define VVHHS   VVHHHSSS
#define VVHHUUU VVHHHUUU
#define VVHHUU  VVHHHUUU
#define VVHHU   VVHHHUUU
#define VVHH    VVHHHSSS
#define VVHSSS  VVHHHSSS
#define VVHSS   VVHHHSSS
#define VVHS    VVHHHSSS
#define VVHUUU  VVHHHUUU
#define VVHUU   VVHHHUUU
#define VVHU    VVHHHUUU
#define VVH     VVHHHSSS
#define VVBBBSS VVBBBSSS
#define VVBBBS  VVBBBSSS
#define VVBBBUU VVBBBUUU
#define VVBBBU  VVBBBUUU
#define VVBBB   VVBBBSSS
#define VVBBSSS VVBBBSSS
#define VVBBSS  VVBBBSSS
#define VVBBS   VVBBBSSS
#define VVBBUUU VVBBBUUU
#define VVBBUU  VVBBBUUU
#define VVBBU   VVBBBUUU
#define VVBB    VVBBBSSS
#define VVBSSS  VVBBBSSS
#define VVBSS   VVBBBSSS
#define VVBS    VVBBBSSS
#define VVBUUU  VVBBBUUU
#define VVBUU   VVBBBUUU
#define VVBU    VVBBBUUU
#define VVB     VVBBBSSS
#define SVWWWSS SVWWWSSS
#define SVWWWS  SVWWWSSS
#define SVWWWUU SVWWWUUU
#define SVWWWU  SVWWWUUU
#define SVWWW   SVWWWSSS
#define SVWWSSS SVWWWSSS
#define SVWWSS  SVWWWSSS
#define SVWWS   SVWWWSSS
#define SVWWUUU SVWWWUUU
#define SVWWUU  SVWWWUUU
#define SVWWU   SVWWWUUU
#define SVWW    SVWWWSSS
#define SVWSSS  SVWWWSSS
#define SVWSS   SVWWWSSS
#define SVWS    SVWWWSSS
#define SVWUUU  SVWWWUUU
#define SVWUU   SVWWWUUU
#define SVWU    SVWWWUUU
#define SVW     SVWWWSSS
#define SVHHHSS SVHHHSSS
#define SVHHHS  SVHHHSSS
#define SVHHHUU SVHHHUUU
#define SVHHHU  SVHHHUUU
#define SVHHH   SVHHHSSS
#define SVHHSSS SVHHHSSS
#define SVHHSS  SVHHHSSS
#define SVHHS   SVHHHSSS
#define SVHHUUU SVHHHUUU
#define SVHHUU  SVHHHUUU
#define SVHHU   SVHHHUUU
#define SVHH    SVHHHSSS
#define SVHSSS  SVHHHSSS
#define SVHSS   SVHHHSSS
#define SVHS    SVHHHSSS
#define SVHUUU  SVHHHUUU
#define SVHUU   SVHHHUUU
#define SVHU    SVHHHUUU
#define SVH     SVHHHSSS
#define SVBBBSS SVBBBSSS
#define SVBBBS  SVBBBSSS
#define SVBBBUU SVBBBUUU
#define SVBBBU  SVBBBUUU
#define SVBBB   SVBBBSSS
#define SVBBSSS SVBBBSSS
#define SVBBSS  SVBBBSSS
#define SVBBS   SVBBBSSS
#define SVBBUUU SVBBBUUU
#define SVBBUU  SVBBBUUU
#define SVBBU   SVBBBUUU
#define SVBB    SVBBBSSS
#define SVBSSS  SVBBBSSS
#define SVBSS   SVBBBSSS
#define SVBS    SVBBBSSS
#define SVBUUU  SVBBBUUU
#define SVBUU   SVBBBUUU
#define SVBU    SVBBBUUU
#define SVB     SVBBBSSS
#define VSWWWSS VSWWWSSS
#define VSWWWS  VSWWWSSS
#define VSWWWUU VSWWWUUU
#define VSWWWU  VSWWWUUU
#define VSWWW   VSWWWSSS
#define VSWWSSS VSWWWSSS
#define VSWWSS  VSWWWSSS
#define VSWWS   VSWWWSSS
#define VSWWUUU VSWWWUUU
#define VSWWUU  VSWWWUUU
#define VSWWU   VSWWWUUU
#define VSWW    VSWWWSSS
#define VSWSSS  VSWWWSSS
#define VSWSS   VSWWWSSS
#define VSWS    VSWWWSSS
#define VSWUUU  VSWWWUUU
#define VSWUU   VSWWWUUU
#define VSWU    VSWWWUUU
#define VSW     VSWWWSSS
#define VSHHHSS VSHHHSSS
#define VSHHHS  VSHHHSSS
#define VSHHHUU VSHHHUUU
#define VSHHHU  VSHHHUUU
#define VSHHH   VSHHHSSS
#define VSHHSSS VSHHHSSS
#define VSHHSS  VSHHHSSS
#define VSHHS   VSHHHSSS
#define VSHHUUU VSHHHUUU
#define VSHHUU  VSHHHUUU
#define VSHHU   VSHHHUUU
#define VSHH    VSHHHSSS
#define VSHSSS  VSHHHSSS
#define VSHSS   VSHHHSSS
#define VSHS    VSHHHSSS
#define VSHUUU  VSHHHUUU
#define VSHUU   VSHHHUUU
#define VSHU    VSHHHUUU
#define VSH     VSHHHSSS
#define VSBBBSS VSBBBSSS
#define VSBBBS  VSBBBSSS
#define VSBBBUU VSBBBUUU
#define VSBBBU  VSBBBUUU
#define VSBBB   VSBBBSSS
#define VSBBSSS VSBBBSSS
#define VSBBSS  VSBBBSSS
#define VSBBS   VSBBBSSS
#define VSBBUUU VSBBBUUU
#define VSBBUU  VSBBBUUU
#define VSBBU   VSBBBUUU
#define VSBB    VSBBBSSS
#define VSBSSS  VSBBBSSS
#define VSBSS   VSBBBSSS
#define VSBS    VSBBBSSS
#define VSBUUU  VSBBBUUU
#define VSBUU   VSBBBUUU
#define VSBU    VSBBBUUU
#define VSB     VSBBBSSS
#define VEWWWSS VEWWWSSS
#define VEWWWS  VEWWWSSS
#define VEWWWUU VEWWWUUU
#define VEWWWU  VEWWWUUU
#define VEWWW   VEWWWSSS
#define VEWWSSS VEWWWSSS
#define VEWWSS  VEWWWSSS
#define VEWWS   VEWWWSSS
#define VEWWUUU VEWWWUUU
#define VEWWUU  VEWWWUUU
#define VEWWU   VEWWWUUU
#define VEWW    VEWWWSSS
#define VEWSSS  VEWWWSSS
#define VEWSS   VEWWWSSS
#define VEWS    VEWWWSSS
#define VEWUUU  VEWWWUUU
#define VEWUU   VEWWWUUU
#define VEWU    VEWWWUUU
#define VEW     VEWWWSSS
#define VEHHHSS VEHHHSSS
#define VEHHHS  VEHHHSSS
#define VEHHHUU VEHHHUUU
#define VEHHHU  VEHHHUUU
#define VEHHH   VEHHHSSS
#define VEHHSSS VEHHHSSS
#define VEHHSS  VEHHHSSS
#define VEHHS   VEHHHSSS
#define VEHHUUU VEHHHUUU
#define VEHHUU  VEHHHUUU
#define VEHHU   VEHHHUUU
#define VEHH    VEHHHSSS
#define VEHSSS  VEHHHSSS
#define VEHSS   VEHHHSSS
#define VEHS    VEHHHSSS
#define VEHUUU  VEHHHUUU
#define VEHUU   VEHHHUUU
#define VEHU    VEHHHUUU
#define VEH     VEHHHSSS
#define VEBBBSS VEBBBSSS
#define VEBBBS  VEBBBSSS
#define VEBBBUU VEBBBUUU
#define VEBBBU  VEBBBUUU
#define VEBBB   VEBBBSSS
#define VEBBSSS VEBBBSSS
#define VEBBSS  VEBBBSSS
#define VEBBS   VEBBBSSS
#define VEBBUUU VEBBBUUU
#define VEBBUU  VEBBBUUU
#define VEBBU   VEBBBUUU
#define VEBB    VEBBBSSS
#define VEBSSS  VEBBBSSS
#define VEBSS   VEBBBSSS
#define VEBS    VEBBBSSS
#define VEBUUU  VEBBBUUU
#define VEBUU   VEBBBUUU
#define VEBU    VEBBBUUU
#define VEB     VEBBBSSS
#define SEWWWSS SEWWWSSS
#define SEWWWS  SEWWWSSS
#define SEWWWUU SEWWWUUU
#define SEWWWU  SEWWWUUU
#define SEWWW   SEWWWSSS
#define SEWWSSS SEWWWSSS
#define SEWWSS  SEWWWSSS
#define SEWWS   SEWWWSSS
#define SEWWUUU SEWWWUUU
#define SEWWUU  SEWWWUUU
#define SEWWU   SEWWWUUU
#define SEWW    SEWWWSSS
#define SEWSSS  SEWWWSSS
#define SEWSS   SEWWWSSS
#define SEWS    SEWWWSSS
#define SEWUUU  SEWWWUUU
#define SEWUU   SEWWWUUU
#define SEWU    SEWWWUUU
#define SEW     SEWWWSSS
#define SEHHHSS SEHHHSSS
#define SEHHHS  SEHHHSSS
#define SEHHHUU SEHHHUUU
#define SEHHHU  SEHHHUUU
#define SEHHH   SEHHHSSS
#define SEHHSSS SEHHHSSS
#define SEHHSS  SEHHHSSS
#define SEHHS   SEHHHSSS
#define SEHHUUU SEHHHUUU
#define SEHHUU  SEHHHUUU
#define SEHHU   SEHHHUUU
#define SEHH    SEHHHSSS
#define SEHSSS  SEHHHSSS
#define SEHSS   SEHHHSSS
#define SEHS    SEHHHSSS
#define SEHUUU  SEHHHUUU
#define SEHUU   SEHHHUUU
#define SEHU    SEHHHUUU
#define SEH     SEHHHSSS
#define SEBBBSS SEBBBSSS
#define SEBBBS  SEBBBSSS
#define SEBBBUU SEBBBUUU
#define SEBBBU  SEBBBUUU
#define SEBBB   SEBBBSSS
#define SEBBSSS SEBBBSSS
#define SEBBSS  SEBBBSSS
#define SEBBS   SEBBBSSS
#define SEBBUUU SEBBBUUU
#define SEBBUU  SEBBBUUU
#define SEBBU   SEBBBUUU
#define SEBB    SEBBBSSS
#define SEBSSS  SEBBBSSS
#define SEBSS   SEBBBSSS
#define SEBS    SEBBBSSS
#define SEBUUU  SEBBBUUU
#define SEBUU   SEBBBUUU
#define SEBU    SEBBBUUU
#define SEB     SEBBBSSS

__attribute__((always_inline)) static inline void vbx_VVWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWWWSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWWWSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWWWSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWWWSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWWWSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWWWSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWWWSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWWWSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWWWSSS, VMULH, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWWWSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWWWSSS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWWWSSS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWWWSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWWWSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWWWSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWWWSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWWWSSS, VSRA, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWWWSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWWWSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWWWSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWWWSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWWWSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWWWSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVWWWSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWWWSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWWWSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWWWSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWWWSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWWWSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWWWSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWWWSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWWWSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWWWSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWWWSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWWWSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWWWSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWWWSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWWWSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWWWSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWWWSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWWWSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWWWSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWWWSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWWWSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWWWSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWWWSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWWWSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWWWSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWWWSSS, VMULHU, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWWWSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWWWSSS, VSLTU, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWWWSSS, VSGTU, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWWWSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWWWSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWWWSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWWWSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWWWSSS, VSRL, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWWWSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWWWSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWWWSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWWWSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWWWSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWWWSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVWWWSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVWWWSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWWWSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWWWSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWWWSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWWWSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWWWSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWWWSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWWWSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWWWSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWWWSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWWWSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWWWSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWWWSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWWWSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWWWSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWWWSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWWWSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHHHSSS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHHHSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHHHSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHHHSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHHHSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHHHSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHHHSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHHHSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHHHSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHHHSSS, VMULH, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHHHSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHHHSSS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHHHSSS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHHHSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHHHSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHHHSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHHHSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHHHSSS, VSRA, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHHHSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHHHSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHHHSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHHHSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHHHSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHHHSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHHHSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVHHHSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHHHSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHHHSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHHHSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHHHSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHHHSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHHHSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHHHSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHHHSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHHHSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHHHSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHHHSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHHHSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHHHSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHHHSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHHHSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHHHSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHHHUUU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHHHSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHHHSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHHHSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHHHSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHHHSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHHHSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHHHSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHHHSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHHHSSS, VMULHU, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHHHSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHHHSSS, VSLTU, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHHHSSS, VSGTU, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHHHSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHHHSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHHHSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHHHSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHHHSSS, VSRL, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHHHSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHHHSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHHHSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHHHSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHHHSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHHHSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHHHSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVHHHSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVHHHSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHHHSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHHHSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHHHSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHHHSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHHHSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHHHSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHHHSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHHHSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHHHSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHHHSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHHHSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHHHSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHHHSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHHHSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHHHSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHHHSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBBBSSS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBBBSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBBBSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBBBSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBBBSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBBBSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBBBSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBBBSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBBBSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBBBSSS, VMULH, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBBBSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBBBSSS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBBBSSS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBBBSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBBBSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBBBSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBBBSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBBBSSS, VSRA, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBBBSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBBBSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBBBSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBBBSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBBBSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBBBSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBBBSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVBBBSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBBBSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBBBSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBBBSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBBBSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBBBSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBBBSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBBBSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBBBSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBBBSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBBBSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBBBSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBBBSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBBBSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBBBSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBBBSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBBBSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBBBUUU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBBBSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBBBSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBBBSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBBBSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBBBSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBBBSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBBBSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBBBSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBBBSSS, VMULHU, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBBBSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBBBSSS, VSLTU, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBBBSSS, VSGTU, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBBBSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBBBSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBBBSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBBBSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBBBSSS, VSRL, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBBBSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBBBSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBBBSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBBBSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBBBSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBBBSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBBBSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_LEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_GTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_LTZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_GEZ, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VVBBBSSS, VSET_MSK_Z, v_out, v_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VVBBBSSS, VSET_MSK_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBBBSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBBBSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBBBSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBBBSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBBBSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBBBSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBBBSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBBBSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBBBSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBBBSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBBBSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBBBSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBBBSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBBBSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBBBSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBBBSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULH, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWWWSSS, VSRA, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVWWWSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWWWSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWWWSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWWWSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWWWSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWWWSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWWWSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWWWSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWWWSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWWWSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWWWSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWWWSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWWWSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWWWSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWWWSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWWWSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWWWSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULHU, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS, VSLTU, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS, VSGTU, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWWWSSS, VSRL, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVWWWSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWWWSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWWWSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWWWSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWWWSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWWWSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWWWSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWWWSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWWWSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWWWSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWWWSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWWWSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWWWSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWWWSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWWWSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWWWSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWWWSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHHHSSS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHHHSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHHHSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHHHSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHHHSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHHHSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHHHSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHHHSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHHHSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHHHSSS, VMULH, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHHHSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHHHSSS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHHHSSS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHHHSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHHHSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHHHSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHHHSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHHHSSS, VSRA, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHHHSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHHHSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHHHSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHHHSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHHHSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHHHSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHHHSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVHHHSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHHHSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHHHSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHHHSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHHHSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHHHSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHHHSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHHHSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHHHSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHHHSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHHHSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHHHSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHHHSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHHHSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHHHSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHHHSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHHHSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHHHUUU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHHHSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHHHSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHHHSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHHHSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHHHSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHHHSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHHHSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHHHSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHHHSSS, VMULHU, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHHHSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHHHSSS, VSLTU, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHHHSSS, VSGTU, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHHHSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHHHSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHHHSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHHHSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHHHSSS, VSRL, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHHHSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHHHSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHHHSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHHHSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHHHSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHHHSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHHHSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVHHHSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHHHSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHHHSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHHHSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHHHSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHHHSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHHHSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHHHSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHHHSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHHHSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHHHSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHHHSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHHHSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHHHSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHHHSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHHHSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHHHSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBBBSSS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBBBSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBBBSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBBBSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBBBSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBBBSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBBBSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBBBSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBBBSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBBBSSS, VMULH, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBBBSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBBBSSS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBBBSSS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBBBSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBBBSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBBBSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBBBSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBBBSSS, VSRA, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBBBSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBBBSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBBBSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBBBSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBBBSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBBBSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBBBSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVBBBSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBBBSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBBBSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBBBSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBBBSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBBBSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBBBSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBBBSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBBBSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBBBSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBBBSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBBBSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBBBSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBBBSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBBBSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBBBSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBBBSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBBBUUU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBBBSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBBBSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBBBSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBBBSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBBBSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBBBSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBBBSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBBBSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBBBSSS, VMULHU, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBBBSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBBBSSS, VSLTU, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBBBSSS, VSGTU, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBBBSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBBBSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBBBSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBBBSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBBBSSS, VSRL, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBBBSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBBBSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBBBSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBBBSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBBBSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBBBSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBBBSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LTZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GEZ, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVBBBSSS, VSET_MSK_Z, v_out, s_in1, v_in2 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBBBSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBBBSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBBBSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBBBSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBBBSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBBBSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBBBSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBBBSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBBBSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBBBSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBBBSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBBBSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBBBSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBBBSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBBBSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBBBSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULH, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWWWSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVW,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVWWWSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULHU, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS,VSGTU, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS,VSLTU, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWWWSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVW,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVWWWSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVWWWSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHHHSSS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHHHSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHHHSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHHHSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHHHSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHHHSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHHHSSS, VMULH, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHHHSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHHHSSS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHHHSSS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHHHSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHHHSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHHHSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHHHSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHHHSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVH,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHHHSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHHHSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHHHSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHHHSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHHHSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHHHSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHHHSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVHHHSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHHHUUU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHHHSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHHHSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHHHSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHHHSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHHHSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHHHSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHHHSSS, VMULHU, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHHHSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHHHSSS,VSGTU, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHHHSSS,VSLTU, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHHHSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHHHSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHHHSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHHHSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHHHSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVH,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHHHSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHHHSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHHHSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHHHSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHHHSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHHHSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHHHSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVHHHSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVHHHSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHHHSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBBBSSS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBBBSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBBBSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBBBSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBBBSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBBBSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBBBSSS, VMULH, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBBBSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBBBSSS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBBBSSS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBBBSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBBBSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBBBSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBBBSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBBBSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVB,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBBBSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBBBSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBBBSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBBBSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBBBSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBBBSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBBBSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVBBBSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBBBUUU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBBBSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBBBSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBBBSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBBBSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBBBSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBBBSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBBBSSS, VMULHU, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBBBSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBBBSSS,VSGTU, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBBBSSS,VSLTU, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBBBSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBBBSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBBBSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBBBSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBBBSSS,VMULHUS,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}	else {vbxasm(modify, SVB,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBBBSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBBBSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBBBSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBBBSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBBBSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBBBSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBBBSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_LTZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_GEZ, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SVBBBSSS, VSET_MSK_Z, v_out, s_in2, v_in1 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SVBBBSSS, VSET_MSK_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBBBSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWWWSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWWWSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWWWSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWWWSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWWWSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWWWSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWWWSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWWWSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWWWSSS, VMULH, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWWWSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWWWSSS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWWWSSS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWWWSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWWWSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWWWSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWWWSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWWWSSS, VSRA, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWWWSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWWWSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWWWSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWWWSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWWWSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWWWSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEWWWSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWWWSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWWWSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWWWSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWWWSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWWWSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWWWSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWWWSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWWWSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWWWSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWWWSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWWWSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWWWSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWWWSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWWWSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWWWSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWWWSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWWWSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWWWSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWWWSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWWWSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWWWSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWWWSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWWWSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWWWSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWWWSSS, VMULHU, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWWWSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWWWSSS, VSLTU, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWWWSSS, VSGTU, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWWWSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWWWSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWWWSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWWWSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWWWSSS, VSRL, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWWWSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWWWSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWWWSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWWWSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWWWSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWWWSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEWWWSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEWWWSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWWWSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWWWSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWWWSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWWWSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWWWSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWWWSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWWWSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWWWSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWWWSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWWWSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWWWSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWWWSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWWWSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWWWSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWWWSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWWWSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHHHSSS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHHHSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHHHSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHHHSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHHHSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHHHSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHHHSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHHHSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHHHSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHHHSSS, VMULH, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHHHSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHHHSSS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHHHSSS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHHHSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHHHSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHHHSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHHHSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHHHSSS, VSRA, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHHHSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHHHSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHHHSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHHHSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHHHSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHHHSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHHHSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEHHHSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHHHSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHHHSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHHHSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHHHSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHHHSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHHHSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHHHSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHHHSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHHHSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHHHSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHHHSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHHHSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHHHSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHHHSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHHHSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHHHSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHHHUUU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHHHSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHHHSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHHHSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHHHSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHHHSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHHHSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHHHSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHHHSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHHHSSS, VMULHU, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHHHSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHHHSSS, VSLTU, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHHHSSS, VSGTU, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHHHSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHHHSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHHHSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHHHSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHHHSSS, VSRL, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHHHSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHHHSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHHHSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHHHSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHHHSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHHHSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHHHSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEHHHSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEHHHSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHHHSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHHHSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHHHSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHHHSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHHHSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHHHSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHHHSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHHHSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHHHSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHHHSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHHHSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHHHSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHHHSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHHHSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHHHSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHHHSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBBBSSS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBBBSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBBBSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBBBSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBBBSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBBBSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBBBSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBBBSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBBBSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBBBSSS, VMULH, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBBBSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBBBSSS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBBBSSS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBBBSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBBBSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBBBSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBBBSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBBBSSS, VSRA, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBBBSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBBBSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBBBSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBBBSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBBBSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBBBSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBBBSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEBBBSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBBBSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBBBSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBBBSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBBBSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBBBSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBBBSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBBBSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBBBSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBBBSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBBBSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBBBSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBBBSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBBBSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBBBSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBBBSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBBBSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBBBUUU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBBBSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBBBSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBBBSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBBBSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBBBSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBBBSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBBBSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBBBSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBBBSSS, VMULHU, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBBBSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBBBSSS, VSLTU, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBBBSSS, VSGTU, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBBBSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBBBSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBBBSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBBBSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBBBSSS, VSRL, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBBBSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBBBSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBBBSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBBBSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBBBSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBBBSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBBBSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_LEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_GTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_LTZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_GEZ, v_out, v_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, VEBBBSSS, VSET_MSK_Z, v_out, v_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, VEBBBSSS, VSET_MSK_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBBBSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBBBSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBBBSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBBBSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBBBSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBBBSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBBBSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBBBSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBBBSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBBBSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBBBSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBBBSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBBBSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBBBSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBBBSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBBBSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWWWSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWWWSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWWWSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWWWSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWWWSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWWWSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWWWSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWWWSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWWWSSS, VMULH, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWWWSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWWWSSS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWWWSSS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWWWSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWWWSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWWWSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWWWSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWWWSSS, VSRA, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWWWSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWWWSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWWWSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWWWSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWWWSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWWWSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEWWWSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWWWSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWWWSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWWWSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWWWSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWWWSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWWWSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWWWSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWWWSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWWWSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWWWSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWWWSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWWWSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWWWSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWWWSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWWWSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWWWSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWWWSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWWWSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWWWSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWWWSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWWWSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWWWSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWWWSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWWWSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWWWSSS, VMULHU, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWWWSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWWWSSS, VSLTU, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWWWSSS, VSGTU, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWWWSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWWWSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWWWSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWWWSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWWWSSS, VSRL, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWWWSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWWWSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWWWSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWWWSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWWWSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWWWSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEWWWSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEWWWSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWWWSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWWWSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWWWSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWWWSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWWWSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWWWSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWWWSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWWWSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWWWSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWWWSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWWWSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWWWSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWWWSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWWWSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWWWSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWWWSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEHHHSSS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEHHHSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEHHHSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEHHHSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEHHHSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEHHHSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEHHHSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEHHHSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEHHHSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEHHHSSS, VMULH, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEHHHSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEHHHSSS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEHHHSSS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEHHHSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEHHHSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEHHHSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEHHHSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEHHHSSS, VSRA, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEHHHSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEHHHSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEHHHSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEHHHSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEHHHSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEHHHSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEHHHSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEHHHSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEHHHSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEHHHSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEHHHSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEHHHSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEHHHSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEHHHSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEHHHSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEHHHSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEHHHSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEHHHSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEHHHSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEHHHSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEHHHSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEHHHSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEHHHSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEHHHSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEHHHUUU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEHHHSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEHHHSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEHHHSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEHHHSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEHHHSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEHHHSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEHHHSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEHHHSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEHHHSSS, VMULHU, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEHHHSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEHHHSSS, VSLTU, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEHHHSSS, VSGTU, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEHHHSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEHHHSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEHHHSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEHHHSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEHHHSSS, VSRL, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEHHHSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEHHHSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEHHHSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEHHHSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEHHHSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEHHHSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEHHHSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEHHHSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEHHHSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEHHHSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEHHHSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEHHHSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEHHHSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEHHHSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEHHHSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEHHHSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEHHHSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEHHHSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEHHHSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEHHHSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEHHHSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEHHHSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEHHHSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEHHHSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEHHHSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEBBBSSS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEBBBSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEBBBSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEBBBSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEBBBSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEBBBSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEBBBSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEBBBSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEBBBSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEBBBSSS, VMULH, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEBBBSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEBBBSSS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEBBBSSS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEBBBSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEBBBSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEBBBSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEBBBSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEBBBSSS, VSRA, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEBBBSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEBBBSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEBBBSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEBBBSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEBBBSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEBBBSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEBBBSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEBBBSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEBBBSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEBBBSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEBBBSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEBBBSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEBBBSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEBBBSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEBBBSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEBBBSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEBBBSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEBBBSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEBBBSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEBBBSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEBBBSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEBBBSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEBBBSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEBBBSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEBBBUUU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t s_in1, ENUM_PARAM ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEBBBSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEBBBSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEBBBSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEBBBSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEBBBSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEBBBSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEBBBSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEBBBSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEBBBSSS, VMULHU, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEBBBSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEBBBSSS, VSLTU, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEBBBSSS, VSGTU, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEBBBSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEBBBSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEBBBSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEBBBSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEBBBSSS, VSRL, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEBBBSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEBBBSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEBBBSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEBBBSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEBBBSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEBBBSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEBBBSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LEZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_LEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GTZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_GTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_LTZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_LTZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_GEZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_GEZ, v_out, s_in1, 0 );
		break;
	case VSET_MSK_Z:
		vbxasm(modify, SEBBBSSS, VSET_MSK_Z, v_out, s_in1, 0 );
		break;
	case VSET_MSK_NZ:
		vbxasm(modify, SEBBBSSS, VSET_MSK_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEBBBSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEBBBSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEBBBSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEBBBSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEBBBSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEBBBSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEBBBSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEBBBSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEBBBSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEBBBSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEBBBSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEBBBSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEBBBSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEBBBSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEBBBSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEBBBSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}
//


#define _vbx(MODIFY,VMODE,VINSTR,DEST,SRCA,SRCB)  vbx_##VMODE(MODIFY,VINSTR,DEST,SRCA,SRCB)

#define vbx(VMODE,VINSTR,DEST,SRCA,SRCB)            _vbx(MOD_NONE,VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked(VMODE,VINSTR,DEST,SRCA,SRCB)     _vbx(MOD_MASKED,VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_acc(VMODE,VINSTR,DEST,SRCA,SRCB)        _vbx(MOD_ACC,VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked_acc(VMODE,VINSTR,DEST,SRCA,SRCB) _vbx(MOD_MASKED|MOD_ACC,VMODE,VINSTR,DEST,SRCA,SRCB)

#undef ENUM_PARAM
#endif // __VBX_CPROTO_H
