//VBXCOPYRIGHTTAG
#ifndef __VBX_CPROTO_H
#define __VBX_CPROTO_H

#define ENUM_CONV_VVBSBSBS(srcb) (srcb)
#define VVBSBS VVBSBSBS
#define VVBS VVBSBSBS
#define ENUM_CONV_VVHSBSBS(srcb) (srcb)
#define VVHSBS VVHSBSBS
#define ENUM_CONV_VVWSBSBS(srcb) (srcb)
#define VVWSBS VVWSBSBS
#define ENUM_CONV_VVBSBSHS(srcb) (srcb)
#define ENUM_CONV_VVHSBSHS(srcb) (srcb)
#define ENUM_CONV_VVWSBSHS(srcb) (srcb)
#define ENUM_CONV_VVBSBSWS(srcb) (srcb)
#define ENUM_CONV_VVHSBSWS(srcb) (srcb)
#define ENUM_CONV_VVWSBSWS(srcb) (srcb)
#define ENUM_CONV_VVBSHSBS(srcb) (srcb)
#define ENUM_CONV_VVHSHSBS(srcb) (srcb)
#define ENUM_CONV_VVWSHSBS(srcb) (srcb)
#define ENUM_CONV_VVBSHSHS(srcb) (srcb)
#define VVBSHS VVBSHSHS
#define ENUM_CONV_VVHSHSHS(srcb) (srcb)
#define VVHSHS VVHSHSHS
#define VVHS VVHSHSHS
#define ENUM_CONV_VVWSHSHS(srcb) (srcb)
#define VVWSHS VVWSHSHS
#define ENUM_CONV_VVBSHSWS(srcb) (srcb)
#define ENUM_CONV_VVHSHSWS(srcb) (srcb)
#define ENUM_CONV_VVWSHSWS(srcb) (srcb)
#define ENUM_CONV_VVBSWSBS(srcb) (srcb)
#define ENUM_CONV_VVHSWSBS(srcb) (srcb)
#define ENUM_CONV_VVWSWSBS(srcb) (srcb)
#define ENUM_CONV_VVBSWSHS(srcb) (srcb)
#define ENUM_CONV_VVHSWSHS(srcb) (srcb)
#define ENUM_CONV_VVWSWSHS(srcb) (srcb)
#define ENUM_CONV_VVBSWSWS(srcb) (srcb)
#define VVBSWS VVBSWSWS
#define ENUM_CONV_VVHSWSWS(srcb) (srcb)
#define VVHSWS VVHSWSWS
#define ENUM_CONV_VVWSWSWS(srcb) (srcb)
#define VVWSWS VVWSWSWS
#define VVWS VVWSWSWS
#define ENUM_CONV_VVBSBSBU(srcb) (srcb)
#define ENUM_CONV_VVHSBSBU(srcb) (srcb)
#define ENUM_CONV_VVWSBSBU(srcb) (srcb)
#define ENUM_CONV_VVBSBSHU(srcb) (srcb)
#define ENUM_CONV_VVHSBSHU(srcb) (srcb)
#define ENUM_CONV_VVWSBSHU(srcb) (srcb)
#define ENUM_CONV_VVBSBSWU(srcb) (srcb)
#define ENUM_CONV_VVHSBSWU(srcb) (srcb)
#define ENUM_CONV_VVWSBSWU(srcb) (srcb)
#define ENUM_CONV_VVBSHSBU(srcb) (srcb)
#define ENUM_CONV_VVHSHSBU(srcb) (srcb)
#define ENUM_CONV_VVWSHSBU(srcb) (srcb)
#define ENUM_CONV_VVBSHSHU(srcb) (srcb)
#define ENUM_CONV_VVHSHSHU(srcb) (srcb)
#define ENUM_CONV_VVWSHSHU(srcb) (srcb)
#define ENUM_CONV_VVBSHSWU(srcb) (srcb)
#define ENUM_CONV_VVHSHSWU(srcb) (srcb)
#define ENUM_CONV_VVWSHSWU(srcb) (srcb)
#define ENUM_CONV_VVBSWSBU(srcb) (srcb)
#define ENUM_CONV_VVHSWSBU(srcb) (srcb)
#define ENUM_CONV_VVWSWSBU(srcb) (srcb)
#define ENUM_CONV_VVBSWSHU(srcb) (srcb)
#define ENUM_CONV_VVHSWSHU(srcb) (srcb)
#define ENUM_CONV_VVWSWSHU(srcb) (srcb)
#define ENUM_CONV_VVBSWSWU(srcb) (srcb)
#define ENUM_CONV_VVHSWSWU(srcb) (srcb)
#define ENUM_CONV_VVWSWSWU(srcb) (srcb)
#define VVBSBSB VVBSBSBS
#define VVHSBSB VVHSBSBS
#define VVWSBSB VVWSBSBS
#define VVBSBSH VVBSBSHS
#define VVHSBSH VVHSBSHS
#define VVWSBSH VVWSBSHS
#define VVBSBSW VVBSBSWS
#define VVHSBSW VVHSBSWS
#define VVWSBSW VVWSBSWS
#define VVBSHSB VVBSHSBS
#define VVHSHSB VVHSHSBS
#define VVWSHSB VVWSHSBS
#define VVBSHSH VVBSHSHS
#define VVHSHSH VVHSHSHS
#define VVWSHSH VVWSHSHS
#define VVBSHSW VVBSHSWS
#define VVHSHSW VVHSHSWS
#define VVWSHSW VVWSHSWS
#define VVBSWSB VVBSWSBS
#define VVHSWSB VVHSWSBS
#define VVWSWSB VVWSWSBS
#define VVBSWSH VVBSWSHS
#define VVHSWSH VVHSWSHS
#define VVWSWSH VVWSWSHS
#define VVBSWSW VVBSWSWS
#define VVHSWSW VVHSWSWS
#define VVWSWSW VVWSWSWS
#define ENUM_CONV_VVBSBUBS(srcb) (srcb)
#define ENUM_CONV_VVHSBUBS(srcb) (srcb)
#define ENUM_CONV_VVWSBUBS(srcb) (srcb)
#define ENUM_CONV_VVBSBUHS(srcb) (srcb)
#define ENUM_CONV_VVHSBUHS(srcb) (srcb)
#define ENUM_CONV_VVWSBUHS(srcb) (srcb)
#define ENUM_CONV_VVBSBUWS(srcb) (srcb)
#define ENUM_CONV_VVHSBUWS(srcb) (srcb)
#define ENUM_CONV_VVWSBUWS(srcb) (srcb)
#define ENUM_CONV_VVBSHUBS(srcb) (srcb)
#define ENUM_CONV_VVHSHUBS(srcb) (srcb)
#define ENUM_CONV_VVWSHUBS(srcb) (srcb)
#define ENUM_CONV_VVBSHUHS(srcb) (srcb)
#define ENUM_CONV_VVHSHUHS(srcb) (srcb)
#define ENUM_CONV_VVWSHUHS(srcb) (srcb)
#define ENUM_CONV_VVBSHUWS(srcb) (srcb)
#define ENUM_CONV_VVHSHUWS(srcb) (srcb)
#define ENUM_CONV_VVWSHUWS(srcb) (srcb)
#define ENUM_CONV_VVBSWUBS(srcb) (srcb)
#define ENUM_CONV_VVHSWUBS(srcb) (srcb)
#define ENUM_CONV_VVWSWUBS(srcb) (srcb)
#define ENUM_CONV_VVBSWUHS(srcb) (srcb)
#define ENUM_CONV_VVHSWUHS(srcb) (srcb)
#define ENUM_CONV_VVWSWUHS(srcb) (srcb)
#define ENUM_CONV_VVBSWUWS(srcb) (srcb)
#define ENUM_CONV_VVHSWUWS(srcb) (srcb)
#define ENUM_CONV_VVWSWUWS(srcb) (srcb)
#define ENUM_CONV_VVBSBUBU(srcb) (srcb)
#define VVBSBU VVBSBUBU
#define ENUM_CONV_VVHSBUBU(srcb) (srcb)
#define VVHSBU VVHSBUBU
#define ENUM_CONV_VVWSBUBU(srcb) (srcb)
#define VVWSBU VVWSBUBU
#define ENUM_CONV_VVBSBUHU(srcb) (srcb)
#define ENUM_CONV_VVHSBUHU(srcb) (srcb)
#define ENUM_CONV_VVWSBUHU(srcb) (srcb)
#define ENUM_CONV_VVBSBUWU(srcb) (srcb)
#define ENUM_CONV_VVHSBUWU(srcb) (srcb)
#define ENUM_CONV_VVWSBUWU(srcb) (srcb)
#define ENUM_CONV_VVBSHUBU(srcb) (srcb)
#define ENUM_CONV_VVHSHUBU(srcb) (srcb)
#define ENUM_CONV_VVWSHUBU(srcb) (srcb)
#define ENUM_CONV_VVBSHUHU(srcb) (srcb)
#define VVBSHU VVBSHUHU
#define ENUM_CONV_VVHSHUHU(srcb) (srcb)
#define VVHSHU VVHSHUHU
#define ENUM_CONV_VVWSHUHU(srcb) (srcb)
#define VVWSHU VVWSHUHU
#define ENUM_CONV_VVBSHUWU(srcb) (srcb)
#define ENUM_CONV_VVHSHUWU(srcb) (srcb)
#define ENUM_CONV_VVWSHUWU(srcb) (srcb)
#define ENUM_CONV_VVBSWUBU(srcb) (srcb)
#define ENUM_CONV_VVHSWUBU(srcb) (srcb)
#define ENUM_CONV_VVWSWUBU(srcb) (srcb)
#define ENUM_CONV_VVBSWUHU(srcb) (srcb)
#define ENUM_CONV_VVHSWUHU(srcb) (srcb)
#define ENUM_CONV_VVWSWUHU(srcb) (srcb)
#define ENUM_CONV_VVBSWUWU(srcb) (srcb)
#define VVBSWU VVBSWUWU
#define ENUM_CONV_VVHSWUWU(srcb) (srcb)
#define VVHSWU VVHSWUWU
#define ENUM_CONV_VVWSWUWU(srcb) (srcb)
#define VVWSWU VVWSWUWU
#define VVBSBUB VVBSBUBS
#define VVHSBUB VVHSBUBS
#define VVWSBUB VVWSBUBS
#define VVBSBUH VVBSBUHS
#define VVHSBUH VVHSBUHS
#define VVWSBUH VVWSBUHS
#define VVBSBUW VVBSBUWS
#define VVHSBUW VVHSBUWS
#define VVWSBUW VVWSBUWS
#define VVBSHUB VVBSHUBS
#define VVHSHUB VVHSHUBS
#define VVWSHUB VVWSHUBS
#define VVBSHUH VVBSHUHS
#define VVHSHUH VVHSHUHS
#define VVWSHUH VVWSHUHS
#define VVBSHUW VVBSHUWS
#define VVHSHUW VVHSHUWS
#define VVWSHUW VVWSHUWS
#define VVBSWUB VVBSWUBS
#define VVHSWUB VVHSWUBS
#define VVWSWUB VVWSWUBS
#define VVBSWUH VVBSWUHS
#define VVHSWUH VVHSWUHS
#define VVWSWUH VVWSWUHS
#define VVBSWUW VVBSWUWS
#define VVHSWUW VVHSWUWS
#define VVWSWUW VVWSWUWS
#define VVBSBBS VVBSBSBS
#define VVHSBBS VVHSBSBS
#define VVWSBBS VVWSBSBS
#define VVBSBHS VVBSBSHS
#define VVHSBHS VVHSBSHS
#define VVWSBHS VVWSBSHS
#define VVBSBWS VVBSBSWS
#define VVHSBWS VVHSBSWS
#define VVWSBWS VVWSBSWS
#define VVBSHBS VVBSHSBS
#define VVHSHBS VVHSHSBS
#define VVWSHBS VVWSHSBS
#define VVBSHHS VVBSHSHS
#define VVHSHHS VVHSHSHS
#define VVWSHHS VVWSHSHS
#define VVBSHWS VVBSHSWS
#define VVHSHWS VVHSHSWS
#define VVWSHWS VVWSHSWS
#define VVBSWBS VVBSWSBS
#define VVHSWBS VVHSWSBS
#define VVWSWBS VVWSWSBS
#define VVBSWHS VVBSWSHS
#define VVHSWHS VVHSWSHS
#define VVWSWHS VVWSWSHS
#define VVBSWWS VVBSWSWS
#define VVHSWWS VVHSWSWS
#define VVWSWWS VVWSWSWS
#define VVBSBBU VVBSBSBU
#define VVHSBBU VVHSBSBU
#define VVWSBBU VVWSBSBU
#define VVBSBHU VVBSBSHU
#define VVHSBHU VVHSBSHU
#define VVWSBHU VVWSBSHU
#define VVBSBWU VVBSBSWU
#define VVHSBWU VVHSBSWU
#define VVWSBWU VVWSBSWU
#define VVBSHBU VVBSHSBU
#define VVHSHBU VVHSHSBU
#define VVWSHBU VVWSHSBU
#define VVBSHHU VVBSHSHU
#define VVHSHHU VVHSHSHU
#define VVWSHHU VVWSHSHU
#define VVBSHWU VVBSHSWU
#define VVHSHWU VVHSHSWU
#define VVWSHWU VVWSHSWU
#define VVBSWBU VVBSWSBU
#define VVHSWBU VVHSWSBU
#define VVWSWBU VVWSWSBU
#define VVBSWHU VVBSWSHU
#define VVHSWHU VVHSWSHU
#define VVWSWHU VVWSWSHU
#define VVBSWWU VVBSWSWU
#define VVHSWWU VVHSWSWU
#define VVWSWWU VVWSWSWU
#define VVBSBB VVBSBSBS
#define VVBSB VVBSBSBS
#define VVHSBB VVHSBSBS
#define VVHSB VVHSBSBS
#define VVWSBB VVWSBSBS
#define VVWSB VVWSBSBS
#define VVBSBH VVBSBSHS
#define VVHSBH VVHSBSHS
#define VVWSBH VVWSBSHS
#define VVBSBW VVBSBSWS
#define VVHSBW VVHSBSWS
#define VVWSBW VVWSBSWS
#define VVBSHB VVBSHSBS
#define VVHSHB VVHSHSBS
#define VVWSHB VVWSHSBS
#define VVBSHH VVBSHSHS
#define VVBSH VVBSHSHS
#define VVHSHH VVHSHSHS
#define VVHSH VVHSHSHS
#define VVWSHH VVWSHSHS
#define VVWSH VVWSHSHS
#define VVBSHW VVBSHSWS
#define VVHSHW VVHSHSWS
#define VVWSHW VVWSHSWS
#define VVBSWB VVBSWSBS
#define VVHSWB VVHSWSBS
#define VVWSWB VVWSWSBS
#define VVBSWH VVBSWSHS
#define VVHSWH VVHSWSHS
#define VVWSWH VVWSWSHS
#define VVBSWW VVBSWSWS
#define VVBSW VVBSWSWS
#define VVHSWW VVHSWSWS
#define VVHSW VVHSWSWS
#define VVWSWW VVWSWSWS
#define VVWSW VVWSWSWS
#define ENUM_CONV_VVBUBSBS(srcb) (srcb)
#define VVBUBS VVBUBSBS
#define ENUM_CONV_VVHUBSBS(srcb) (srcb)
#define VVHUBS VVHUBSBS
#define ENUM_CONV_VVWUBSBS(srcb) (srcb)
#define VVWUBS VVWUBSBS
#define ENUM_CONV_VVBUBSHS(srcb) (srcb)
#define ENUM_CONV_VVHUBSHS(srcb) (srcb)
#define ENUM_CONV_VVWUBSHS(srcb) (srcb)
#define ENUM_CONV_VVBUBSWS(srcb) (srcb)
#define ENUM_CONV_VVHUBSWS(srcb) (srcb)
#define ENUM_CONV_VVWUBSWS(srcb) (srcb)
#define ENUM_CONV_VVBUHSBS(srcb) (srcb)
#define ENUM_CONV_VVHUHSBS(srcb) (srcb)
#define ENUM_CONV_VVWUHSBS(srcb) (srcb)
#define ENUM_CONV_VVBUHSHS(srcb) (srcb)
#define VVBUHS VVBUHSHS
#define ENUM_CONV_VVHUHSHS(srcb) (srcb)
#define VVHUHS VVHUHSHS
#define ENUM_CONV_VVWUHSHS(srcb) (srcb)
#define VVWUHS VVWUHSHS
#define ENUM_CONV_VVBUHSWS(srcb) (srcb)
#define ENUM_CONV_VVHUHSWS(srcb) (srcb)
#define ENUM_CONV_VVWUHSWS(srcb) (srcb)
#define ENUM_CONV_VVBUWSBS(srcb) (srcb)
#define ENUM_CONV_VVHUWSBS(srcb) (srcb)
#define ENUM_CONV_VVWUWSBS(srcb) (srcb)
#define ENUM_CONV_VVBUWSHS(srcb) (srcb)
#define ENUM_CONV_VVHUWSHS(srcb) (srcb)
#define ENUM_CONV_VVWUWSHS(srcb) (srcb)
#define ENUM_CONV_VVBUWSWS(srcb) (srcb)
#define VVBUWS VVBUWSWS
#define ENUM_CONV_VVHUWSWS(srcb) (srcb)
#define VVHUWS VVHUWSWS
#define ENUM_CONV_VVWUWSWS(srcb) (srcb)
#define VVWUWS VVWUWSWS
#define ENUM_CONV_VVBUBSBU(srcb) (srcb)
#define ENUM_CONV_VVHUBSBU(srcb) (srcb)
#define ENUM_CONV_VVWUBSBU(srcb) (srcb)
#define ENUM_CONV_VVBUBSHU(srcb) (srcb)
#define ENUM_CONV_VVHUBSHU(srcb) (srcb)
#define ENUM_CONV_VVWUBSHU(srcb) (srcb)
#define ENUM_CONV_VVBUBSWU(srcb) (srcb)
#define ENUM_CONV_VVHUBSWU(srcb) (srcb)
#define ENUM_CONV_VVWUBSWU(srcb) (srcb)
#define ENUM_CONV_VVBUHSBU(srcb) (srcb)
#define ENUM_CONV_VVHUHSBU(srcb) (srcb)
#define ENUM_CONV_VVWUHSBU(srcb) (srcb)
#define ENUM_CONV_VVBUHSHU(srcb) (srcb)
#define ENUM_CONV_VVHUHSHU(srcb) (srcb)
#define ENUM_CONV_VVWUHSHU(srcb) (srcb)
#define ENUM_CONV_VVBUHSWU(srcb) (srcb)
#define ENUM_CONV_VVHUHSWU(srcb) (srcb)
#define ENUM_CONV_VVWUHSWU(srcb) (srcb)
#define ENUM_CONV_VVBUWSBU(srcb) (srcb)
#define ENUM_CONV_VVHUWSBU(srcb) (srcb)
#define ENUM_CONV_VVWUWSBU(srcb) (srcb)
#define ENUM_CONV_VVBUWSHU(srcb) (srcb)
#define ENUM_CONV_VVHUWSHU(srcb) (srcb)
#define ENUM_CONV_VVWUWSHU(srcb) (srcb)
#define ENUM_CONV_VVBUWSWU(srcb) (srcb)
#define ENUM_CONV_VVHUWSWU(srcb) (srcb)
#define ENUM_CONV_VVWUWSWU(srcb) (srcb)
#define VVBUBSB VVBUBSBS
#define VVHUBSB VVHUBSBS
#define VVWUBSB VVWUBSBS
#define VVBUBSH VVBUBSHS
#define VVHUBSH VVHUBSHS
#define VVWUBSH VVWUBSHS
#define VVBUBSW VVBUBSWS
#define VVHUBSW VVHUBSWS
#define VVWUBSW VVWUBSWS
#define VVBUHSB VVBUHSBS
#define VVHUHSB VVHUHSBS
#define VVWUHSB VVWUHSBS
#define VVBUHSH VVBUHSHS
#define VVHUHSH VVHUHSHS
#define VVWUHSH VVWUHSHS
#define VVBUHSW VVBUHSWS
#define VVHUHSW VVHUHSWS
#define VVWUHSW VVWUHSWS
#define VVBUWSB VVBUWSBS
#define VVHUWSB VVHUWSBS
#define VVWUWSB VVWUWSBS
#define VVBUWSH VVBUWSHS
#define VVHUWSH VVHUWSHS
#define VVWUWSH VVWUWSHS
#define VVBUWSW VVBUWSWS
#define VVHUWSW VVHUWSWS
#define VVWUWSW VVWUWSWS
#define ENUM_CONV_VVBUBUBS(srcb) (srcb)
#define ENUM_CONV_VVHUBUBS(srcb) (srcb)
#define ENUM_CONV_VVWUBUBS(srcb) (srcb)
#define ENUM_CONV_VVBUBUHS(srcb) (srcb)
#define ENUM_CONV_VVHUBUHS(srcb) (srcb)
#define ENUM_CONV_VVWUBUHS(srcb) (srcb)
#define ENUM_CONV_VVBUBUWS(srcb) (srcb)
#define ENUM_CONV_VVHUBUWS(srcb) (srcb)
#define ENUM_CONV_VVWUBUWS(srcb) (srcb)
#define ENUM_CONV_VVBUHUBS(srcb) (srcb)
#define ENUM_CONV_VVHUHUBS(srcb) (srcb)
#define ENUM_CONV_VVWUHUBS(srcb) (srcb)
#define ENUM_CONV_VVBUHUHS(srcb) (srcb)
#define ENUM_CONV_VVHUHUHS(srcb) (srcb)
#define ENUM_CONV_VVWUHUHS(srcb) (srcb)
#define ENUM_CONV_VVBUHUWS(srcb) (srcb)
#define ENUM_CONV_VVHUHUWS(srcb) (srcb)
#define ENUM_CONV_VVWUHUWS(srcb) (srcb)
#define ENUM_CONV_VVBUWUBS(srcb) (srcb)
#define ENUM_CONV_VVHUWUBS(srcb) (srcb)
#define ENUM_CONV_VVWUWUBS(srcb) (srcb)
#define ENUM_CONV_VVBUWUHS(srcb) (srcb)
#define ENUM_CONV_VVHUWUHS(srcb) (srcb)
#define ENUM_CONV_VVWUWUHS(srcb) (srcb)
#define ENUM_CONV_VVBUWUWS(srcb) (srcb)
#define ENUM_CONV_VVHUWUWS(srcb) (srcb)
#define ENUM_CONV_VVWUWUWS(srcb) (srcb)
#define ENUM_CONV_VVBUBUBU(srcb) (srcb)
#define VVBUBU VVBUBUBU
#define VVBU VVBUBUBU
#define ENUM_CONV_VVHUBUBU(srcb) (srcb)
#define VVHUBU VVHUBUBU
#define ENUM_CONV_VVWUBUBU(srcb) (srcb)
#define VVWUBU VVWUBUBU
#define ENUM_CONV_VVBUBUHU(srcb) (srcb)
#define ENUM_CONV_VVHUBUHU(srcb) (srcb)
#define ENUM_CONV_VVWUBUHU(srcb) (srcb)
#define ENUM_CONV_VVBUBUWU(srcb) (srcb)
#define ENUM_CONV_VVHUBUWU(srcb) (srcb)
#define ENUM_CONV_VVWUBUWU(srcb) (srcb)
#define ENUM_CONV_VVBUHUBU(srcb) (srcb)
#define ENUM_CONV_VVHUHUBU(srcb) (srcb)
#define ENUM_CONV_VVWUHUBU(srcb) (srcb)
#define ENUM_CONV_VVBUHUHU(srcb) (srcb)
#define VVBUHU VVBUHUHU
#define ENUM_CONV_VVHUHUHU(srcb) (srcb)
#define VVHUHU VVHUHUHU
#define VVHU VVHUHUHU
#define ENUM_CONV_VVWUHUHU(srcb) (srcb)
#define VVWUHU VVWUHUHU
#define ENUM_CONV_VVBUHUWU(srcb) (srcb)
#define ENUM_CONV_VVHUHUWU(srcb) (srcb)
#define ENUM_CONV_VVWUHUWU(srcb) (srcb)
#define ENUM_CONV_VVBUWUBU(srcb) (srcb)
#define ENUM_CONV_VVHUWUBU(srcb) (srcb)
#define ENUM_CONV_VVWUWUBU(srcb) (srcb)
#define ENUM_CONV_VVBUWUHU(srcb) (srcb)
#define ENUM_CONV_VVHUWUHU(srcb) (srcb)
#define ENUM_CONV_VVWUWUHU(srcb) (srcb)
#define ENUM_CONV_VVBUWUWU(srcb) (srcb)
#define VVBUWU VVBUWUWU
#define ENUM_CONV_VVHUWUWU(srcb) (srcb)
#define VVHUWU VVHUWUWU
#define ENUM_CONV_VVWUWUWU(srcb) (srcb)
#define VVWUWU VVWUWUWU
#define VVWU VVWUWUWU
#define VVBUBUB VVBUBUBS
#define VVHUBUB VVHUBUBS
#define VVWUBUB VVWUBUBS
#define VVBUBUH VVBUBUHS
#define VVHUBUH VVHUBUHS
#define VVWUBUH VVWUBUHS
#define VVBUBUW VVBUBUWS
#define VVHUBUW VVHUBUWS
#define VVWUBUW VVWUBUWS
#define VVBUHUB VVBUHUBS
#define VVHUHUB VVHUHUBS
#define VVWUHUB VVWUHUBS
#define VVBUHUH VVBUHUHS
#define VVHUHUH VVHUHUHS
#define VVWUHUH VVWUHUHS
#define VVBUHUW VVBUHUWS
#define VVHUHUW VVHUHUWS
#define VVWUHUW VVWUHUWS
#define VVBUWUB VVBUWUBS
#define VVHUWUB VVHUWUBS
#define VVWUWUB VVWUWUBS
#define VVBUWUH VVBUWUHS
#define VVHUWUH VVHUWUHS
#define VVWUWUH VVWUWUHS
#define VVBUWUW VVBUWUWS
#define VVHUWUW VVHUWUWS
#define VVWUWUW VVWUWUWS
#define VVBUBBS VVBUBSBS
#define VVHUBBS VVHUBSBS
#define VVWUBBS VVWUBSBS
#define VVBUBHS VVBUBSHS
#define VVHUBHS VVHUBSHS
#define VVWUBHS VVWUBSHS
#define VVBUBWS VVBUBSWS
#define VVHUBWS VVHUBSWS
#define VVWUBWS VVWUBSWS
#define VVBUHBS VVBUHSBS
#define VVHUHBS VVHUHSBS
#define VVWUHBS VVWUHSBS
#define VVBUHHS VVBUHSHS
#define VVHUHHS VVHUHSHS
#define VVWUHHS VVWUHSHS
#define VVBUHWS VVBUHSWS
#define VVHUHWS VVHUHSWS
#define VVWUHWS VVWUHSWS
#define VVBUWBS VVBUWSBS
#define VVHUWBS VVHUWSBS
#define VVWUWBS VVWUWSBS
#define VVBUWHS VVBUWSHS
#define VVHUWHS VVHUWSHS
#define VVWUWHS VVWUWSHS
#define VVBUWWS VVBUWSWS
#define VVHUWWS VVHUWSWS
#define VVWUWWS VVWUWSWS
#define VVBUBBU VVBUBSBU
#define VVHUBBU VVHUBSBU
#define VVWUBBU VVWUBSBU
#define VVBUBHU VVBUBSHU
#define VVHUBHU VVHUBSHU
#define VVWUBHU VVWUBSHU
#define VVBUBWU VVBUBSWU
#define VVHUBWU VVHUBSWU
#define VVWUBWU VVWUBSWU
#define VVBUHBU VVBUHSBU
#define VVHUHBU VVHUHSBU
#define VVWUHBU VVWUHSBU
#define VVBUHHU VVBUHSHU
#define VVHUHHU VVHUHSHU
#define VVWUHHU VVWUHSHU
#define VVBUHWU VVBUHSWU
#define VVHUHWU VVHUHSWU
#define VVWUHWU VVWUHSWU
#define VVBUWBU VVBUWSBU
#define VVHUWBU VVHUWSBU
#define VVWUWBU VVWUWSBU
#define VVBUWHU VVBUWSHU
#define VVHUWHU VVHUWSHU
#define VVWUWHU VVWUWSHU
#define VVBUWWU VVBUWSWU
#define VVHUWWU VVHUWSWU
#define VVWUWWU VVWUWSWU
#define VVBUBB VVBUBSBS
#define VVBUB VVBUBSBS
#define VVHUBB VVHUBSBS
#define VVHUB VVHUBSBS
#define VVWUBB VVWUBSBS
#define VVWUB VVWUBSBS
#define VVBUBH VVBUBSHS
#define VVHUBH VVHUBSHS
#define VVWUBH VVWUBSHS
#define VVBUBW VVBUBSWS
#define VVHUBW VVHUBSWS
#define VVWUBW VVWUBSWS
#define VVBUHB VVBUHSBS
#define VVHUHB VVHUHSBS
#define VVWUHB VVWUHSBS
#define VVBUHH VVBUHSHS
#define VVBUH VVBUHSHS
#define VVHUHH VVHUHSHS
#define VVHUH VVHUHSHS
#define VVWUHH VVWUHSHS
#define VVWUH VVWUHSHS
#define VVBUHW VVBUHSWS
#define VVHUHW VVHUHSWS
#define VVWUHW VVWUHSWS
#define VVBUWB VVBUWSBS
#define VVHUWB VVHUWSBS
#define VVWUWB VVWUWSBS
#define VVBUWH VVBUWSHS
#define VVHUWH VVHUWSHS
#define VVWUWH VVWUWSHS
#define VVBUWW VVBUWSWS
#define VVBUW VVBUWSWS
#define VVHUWW VVHUWSWS
#define VVHUW VVHUWSWS
#define VVWUWW VVWUWSWS
#define VVWUW VVWUWSWS
#define VVBBSBS VVBSBSBS
#define VVBBS VVBSBSBS
#define VVHBSBS VVHSBSBS
#define VVHBS VVHSBSBS
#define VVWBSBS VVWSBSBS
#define VVWBS VVWSBSBS
#define VVBBSHS VVBSBSHS
#define VVHBSHS VVHSBSHS
#define VVWBSHS VVWSBSHS
#define VVBBSWS VVBSBSWS
#define VVHBSWS VVHSBSWS
#define VVWBSWS VVWSBSWS
#define VVBHSBS VVBSHSBS
#define VVHHSBS VVHSHSBS
#define VVWHSBS VVWSHSBS
#define VVBHSHS VVBSHSHS
#define VVBHS VVBSHSHS
#define VVHHSHS VVHSHSHS
#define VVHHS VVHSHSHS
#define VVWHSHS VVWSHSHS
#define VVWHS VVWSHSHS
#define VVBHSWS VVBSHSWS
#define VVHHSWS VVHSHSWS
#define VVWHSWS VVWSHSWS
#define VVBWSBS VVBSWSBS
#define VVHWSBS VVHSWSBS
#define VVWWSBS VVWSWSBS
#define VVBWSHS VVBSWSHS
#define VVHWSHS VVHSWSHS
#define VVWWSHS VVWSWSHS
#define VVBWSWS VVBSWSWS
#define VVBWS VVBSWSWS
#define VVHWSWS VVHSWSWS
#define VVHWS VVHSWSWS
#define VVWWSWS VVWSWSWS
#define VVWWS VVWSWSWS
#define VVBBSBU VVBSBSBU
#define VVHBSBU VVHSBSBU
#define VVWBSBU VVWSBSBU
#define VVBBSHU VVBSBSHU
#define VVHBSHU VVHSBSHU
#define VVWBSHU VVWSBSHU
#define VVBBSWU VVBSBSWU
#define VVHBSWU VVHSBSWU
#define VVWBSWU VVWSBSWU
#define VVBHSBU VVBSHSBU
#define VVHHSBU VVHSHSBU
#define VVWHSBU VVWSHSBU
#define VVBHSHU VVBSHSHU
#define VVHHSHU VVHSHSHU
#define VVWHSHU VVWSHSHU
#define VVBHSWU VVBSHSWU
#define VVHHSWU VVHSHSWU
#define VVWHSWU VVWSHSWU
#define VVBWSBU VVBSWSBU
#define VVHWSBU VVHSWSBU
#define VVWWSBU VVWSWSBU
#define VVBWSHU VVBSWSHU
#define VVHWSHU VVHSWSHU
#define VVWWSHU VVWSWSHU
#define VVBWSWU VVBSWSWU
#define VVHWSWU VVHSWSWU
#define VVWWSWU VVWSWSWU
#define VVBBSB VVBSBSBS
#define VVHBSB VVHSBSBS
#define VVWBSB VVWSBSBS
#define VVBBSH VVBSBSHS
#define VVHBSH VVHSBSHS
#define VVWBSH VVWSBSHS
#define VVBBSW VVBSBSWS
#define VVHBSW VVHSBSWS
#define VVWBSW VVWSBSWS
#define VVBHSB VVBSHSBS
#define VVHHSB VVHSHSBS
#define VVWHSB VVWSHSBS
#define VVBHSH VVBSHSHS
#define VVHHSH VVHSHSHS
#define VVWHSH VVWSHSHS
#define VVBHSW VVBSHSWS
#define VVHHSW VVHSHSWS
#define VVWHSW VVWSHSWS
#define VVBWSB VVBSWSBS
#define VVHWSB VVHSWSBS
#define VVWWSB VVWSWSBS
#define VVBWSH VVBSWSHS
#define VVHWSH VVHSWSHS
#define VVWWSH VVWSWSHS
#define VVBWSW VVBSWSWS
#define VVHWSW VVHSWSWS
#define VVWWSW VVWSWSWS
#define VVBBUBS VVBSBUBS
#define VVHBUBS VVHSBUBS
#define VVWBUBS VVWSBUBS
#define VVBBUHS VVBSBUHS
#define VVHBUHS VVHSBUHS
#define VVWBUHS VVWSBUHS
#define VVBBUWS VVBSBUWS
#define VVHBUWS VVHSBUWS
#define VVWBUWS VVWSBUWS
#define VVBHUBS VVBSHUBS
#define VVHHUBS VVHSHUBS
#define VVWHUBS VVWSHUBS
#define VVBHUHS VVBSHUHS
#define VVHHUHS VVHSHUHS
#define VVWHUHS VVWSHUHS
#define VVBHUWS VVBSHUWS
#define VVHHUWS VVHSHUWS
#define VVWHUWS VVWSHUWS
#define VVBWUBS VVBSWUBS
#define VVHWUBS VVHSWUBS
#define VVWWUBS VVWSWUBS
#define VVBWUHS VVBSWUHS
#define VVHWUHS VVHSWUHS
#define VVWWUHS VVWSWUHS
#define VVBWUWS VVBSWUWS
#define VVHWUWS VVHSWUWS
#define VVWWUWS VVWSWUWS
#define VVBBUBU VVBSBUBU
#define VVBBU VVBSBUBU
#define VVHBUBU VVHSBUBU
#define VVHBU VVHSBUBU
#define VVWBUBU VVWSBUBU
#define VVWBU VVWSBUBU
#define VVBBUHU VVBSBUHU
#define VVHBUHU VVHSBUHU
#define VVWBUHU VVWSBUHU
#define VVBBUWU VVBSBUWU
#define VVHBUWU VVHSBUWU
#define VVWBUWU VVWSBUWU
#define VVBHUBU VVBSHUBU
#define VVHHUBU VVHSHUBU
#define VVWHUBU VVWSHUBU
#define VVBHUHU VVBSHUHU
#define VVBHU VVBSHUHU
#define VVHHUHU VVHSHUHU
#define VVHHU VVHSHUHU
#define VVWHUHU VVWSHUHU
#define VVWHU VVWSHUHU
#define VVBHUWU VVBSHUWU
#define VVHHUWU VVHSHUWU
#define VVWHUWU VVWSHUWU
#define VVBWUBU VVBSWUBU
#define VVHWUBU VVHSWUBU
#define VVWWUBU VVWSWUBU
#define VVBWUHU VVBSWUHU
#define VVHWUHU VVHSWUHU
#define VVWWUHU VVWSWUHU
#define VVBWUWU VVBSWUWU
#define VVBWU VVBSWUWU
#define VVHWUWU VVHSWUWU
#define VVHWU VVHSWUWU
#define VVWWUWU VVWSWUWU
#define VVWWU VVWSWUWU
#define VVBBUB VVBSBUBS
#define VVHBUB VVHSBUBS
#define VVWBUB VVWSBUBS
#define VVBBUH VVBSBUHS
#define VVHBUH VVHSBUHS
#define VVWBUH VVWSBUHS
#define VVBBUW VVBSBUWS
#define VVHBUW VVHSBUWS
#define VVWBUW VVWSBUWS
#define VVBHUB VVBSHUBS
#define VVHHUB VVHSHUBS
#define VVWHUB VVWSHUBS
#define VVBHUH VVBSHUHS
#define VVHHUH VVHSHUHS
#define VVWHUH VVWSHUHS
#define VVBHUW VVBSHUWS
#define VVHHUW VVHSHUWS
#define VVWHUW VVWSHUWS
#define VVBWUB VVBSWUBS
#define VVHWUB VVHSWUBS
#define VVWWUB VVWSWUBS
#define VVBWUH VVBSWUHS
#define VVHWUH VVHSWUHS
#define VVWWUH VVWSWUHS
#define VVBWUW VVBSWUWS
#define VVHWUW VVHSWUWS
#define VVWWUW VVWSWUWS
#define VVBBBS VVBSBSBS
#define VVHBBS VVHSBSBS
#define VVWBBS VVWSBSBS
#define VVBBHS VVBSBSHS
#define VVHBHS VVHSBSHS
#define VVWBHS VVWSBSHS
#define VVBBWS VVBSBSWS
#define VVHBWS VVHSBSWS
#define VVWBWS VVWSBSWS
#define VVBHBS VVBSHSBS
#define VVHHBS VVHSHSBS
#define VVWHBS VVWSHSBS
#define VVBHHS VVBSHSHS
#define VVHHHS VVHSHSHS
#define VVWHHS VVWSHSHS
#define VVBHWS VVBSHSWS
#define VVHHWS VVHSHSWS
#define VVWHWS VVWSHSWS
#define VVBWBS VVBSWSBS
#define VVHWBS VVHSWSBS
#define VVWWBS VVWSWSBS
#define VVBWHS VVBSWSHS
#define VVHWHS VVHSWSHS
#define VVWWHS VVWSWSHS
#define VVBWWS VVBSWSWS
#define VVHWWS VVHSWSWS
#define VVWWWS VVWSWSWS
#define VVBBBU VVBSBSBU
#define VVHBBU VVHSBSBU
#define VVWBBU VVWSBSBU
#define VVBBHU VVBSBSHU
#define VVHBHU VVHSBSHU
#define VVWBHU VVWSBSHU
#define VVBBWU VVBSBSWU
#define VVHBWU VVHSBSWU
#define VVWBWU VVWSBSWU
#define VVBHBU VVBSHSBU
#define VVHHBU VVHSHSBU
#define VVWHBU VVWSHSBU
#define VVBHHU VVBSHSHU
#define VVHHHU VVHSHSHU
#define VVWHHU VVWSHSHU
#define VVBHWU VVBSHSWU
#define VVHHWU VVHSHSWU
#define VVWHWU VVWSHSWU
#define VVBWBU VVBSWSBU
#define VVHWBU VVHSWSBU
#define VVWWBU VVWSWSBU
#define VVBWHU VVBSWSHU
#define VVHWHU VVHSWSHU
#define VVWWHU VVWSWSHU
#define VVBWWU VVBSWSWU
#define VVHWWU VVHSWSWU
#define VVWWWU VVWSWSWU
#define VVBBB VVBSBSBS
#define VVBB VVBSBSBS
#define VVB VVBSBSBS
#define VVHBB VVHSBSBS
#define VVHB VVHSBSBS
#define VVWBB VVWSBSBS
#define VVWB VVWSBSBS
#define VVBBH VVBSBSHS
#define VVHBH VVHSBSHS
#define VVWBH VVWSBSHS
#define VVBBW VVBSBSWS
#define VVHBW VVHSBSWS
#define VVWBW VVWSBSWS
#define VVBHB VVBSHSBS
#define VVHHB VVHSHSBS
#define VVWHB VVWSHSBS
#define VVBHH VVBSHSHS
#define VVBH VVBSHSHS
#define VVHHH VVHSHSHS
#define VVHH VVHSHSHS
#define VVH VVHSHSHS
#define VVWHH VVWSHSHS
#define VVWH VVWSHSHS
#define VVBHW VVBSHSWS
#define VVHHW VVHSHSWS
#define VVWHW VVWSHSWS
#define VVBWB VVBSWSBS
#define VVHWB VVHSWSBS
#define VVWWB VVWSWSBS
#define VVBWH VVBSWSHS
#define VVHWH VVHSWSHS
#define VVWWH VVWSWSHS
#define VVBWW VVBSWSWS
#define VVBW VVBSWSWS
#define VVHWW VVHSWSWS
#define VVHW VVHSWSWS
#define VVWWW VVWSWSWS
#define VVWW VVWSWSWS
#define VVW VVWSWSWS
#define ENUM_CONV_SVBSBSBS(srcb) (srcb)
#define SVBSBS SVBSBSBS
#define SVBS SVBSBSBS
#define ENUM_CONV_SVHSBSBS(srcb) (srcb)
#define SVHSBS SVHSBSBS
#define ENUM_CONV_SVWSBSBS(srcb) (srcb)
#define SVWSBS SVWSBSBS
#define ENUM_CONV_SVBSHSHS(srcb) (srcb)
#define SVBSHS SVBSHSHS
#define ENUM_CONV_SVHSHSHS(srcb) (srcb)
#define SVHSHS SVHSHSHS
#define SVHS SVHSHSHS
#define ENUM_CONV_SVWSHSHS(srcb) (srcb)
#define SVWSHS SVWSHSHS
#define ENUM_CONV_SVBSWSWS(srcb) (srcb)
#define SVBSWS SVBSWSWS
#define ENUM_CONV_SVHSWSWS(srcb) (srcb)
#define SVHSWS SVHSWSWS
#define ENUM_CONV_SVWSWSWS(srcb) (srcb)
#define SVWSWS SVWSWSWS
#define SVWS SVWSWSWS
#define SVBSBSB SVBSBSBS
#define SVHSBSB SVHSBSBS
#define SVWSBSB SVWSBSBS
#define SVBSHSH SVBSHSHS
#define SVHSHSH SVHSHSHS
#define SVWSHSH SVWSHSHS
#define SVBSWSW SVBSWSWS
#define SVHSWSW SVHSWSWS
#define SVWSWSW SVWSWSWS
#define ENUM_CONV_SVBSBUBU(srcb) (srcb)
#define SVBSBU SVBSBUBU
#define ENUM_CONV_SVHSBUBU(srcb) (srcb)
#define SVHSBU SVHSBUBU
#define ENUM_CONV_SVWSBUBU(srcb) (srcb)
#define SVWSBU SVWSBUBU
#define ENUM_CONV_SVBSHUHU(srcb) (srcb)
#define SVBSHU SVBSHUHU
#define ENUM_CONV_SVHSHUHU(srcb) (srcb)
#define SVHSHU SVHSHUHU
#define ENUM_CONV_SVWSHUHU(srcb) (srcb)
#define SVWSHU SVWSHUHU
#define ENUM_CONV_SVBSWUWU(srcb) (srcb)
#define SVBSWU SVBSWUWU
#define ENUM_CONV_SVHSWUWU(srcb) (srcb)
#define SVHSWU SVHSWUWU
#define ENUM_CONV_SVWSWUWU(srcb) (srcb)
#define SVWSWU SVWSWUWU
#define SVBSBBS SVBSBSBS
#define SVHSBBS SVHSBSBS
#define SVWSBBS SVWSBSBS
#define SVBSHHS SVBSHSHS
#define SVHSHHS SVHSHSHS
#define SVWSHHS SVWSHSHS
#define SVBSWWS SVBSWSWS
#define SVHSWWS SVHSWSWS
#define SVWSWWS SVWSWSWS
#define SVBSBB SVBSBSBS
#define SVBSB SVBSBSBS
#define SVHSBB SVHSBSBS
#define SVHSB SVHSBSBS
#define SVWSBB SVWSBSBS
#define SVWSB SVWSBSBS
#define SVBSHH SVBSHSHS
#define SVBSH SVBSHSHS
#define SVHSHH SVHSHSHS
#define SVHSH SVHSHSHS
#define SVWSHH SVWSHSHS
#define SVWSH SVWSHSHS
#define SVBSWW SVBSWSWS
#define SVBSW SVBSWSWS
#define SVHSWW SVHSWSWS
#define SVHSW SVHSWSWS
#define SVWSWW SVWSWSWS
#define SVWSW SVWSWSWS
#define ENUM_CONV_SVBUBSBS(srcb) (srcb)
#define SVBUBS SVBUBSBS
#define ENUM_CONV_SVHUBSBS(srcb) (srcb)
#define SVHUBS SVHUBSBS
#define ENUM_CONV_SVWUBSBS(srcb) (srcb)
#define SVWUBS SVWUBSBS
#define ENUM_CONV_SVBUHSHS(srcb) (srcb)
#define SVBUHS SVBUHSHS
#define ENUM_CONV_SVHUHSHS(srcb) (srcb)
#define SVHUHS SVHUHSHS
#define ENUM_CONV_SVWUHSHS(srcb) (srcb)
#define SVWUHS SVWUHSHS
#define ENUM_CONV_SVBUWSWS(srcb) (srcb)
#define SVBUWS SVBUWSWS
#define ENUM_CONV_SVHUWSWS(srcb) (srcb)
#define SVHUWS SVHUWSWS
#define ENUM_CONV_SVWUWSWS(srcb) (srcb)
#define SVWUWS SVWUWSWS
#define SVBUBSB SVBUBSBS
#define SVHUBSB SVHUBSBS
#define SVWUBSB SVWUBSBS
#define SVBUHSH SVBUHSHS
#define SVHUHSH SVHUHSHS
#define SVWUHSH SVWUHSHS
#define SVBUWSW SVBUWSWS
#define SVHUWSW SVHUWSWS
#define SVWUWSW SVWUWSWS
#define ENUM_CONV_SVBUBUBU(srcb) (srcb)
#define SVBUBU SVBUBUBU
#define SVBU SVBUBUBU
#define ENUM_CONV_SVHUBUBU(srcb) (srcb)
#define SVHUBU SVHUBUBU
#define ENUM_CONV_SVWUBUBU(srcb) (srcb)
#define SVWUBU SVWUBUBU
#define ENUM_CONV_SVBUHUHU(srcb) (srcb)
#define SVBUHU SVBUHUHU
#define ENUM_CONV_SVHUHUHU(srcb) (srcb)
#define SVHUHU SVHUHUHU
#define SVHU SVHUHUHU
#define ENUM_CONV_SVWUHUHU(srcb) (srcb)
#define SVWUHU SVWUHUHU
#define ENUM_CONV_SVBUWUWU(srcb) (srcb)
#define SVBUWU SVBUWUWU
#define ENUM_CONV_SVHUWUWU(srcb) (srcb)
#define SVHUWU SVHUWUWU
#define ENUM_CONV_SVWUWUWU(srcb) (srcb)
#define SVWUWU SVWUWUWU
#define SVWU SVWUWUWU
#define SVBUBBS SVBUBSBS
#define SVHUBBS SVHUBSBS
#define SVWUBBS SVWUBSBS
#define SVBUHHS SVBUHSHS
#define SVHUHHS SVHUHSHS
#define SVWUHHS SVWUHSHS
#define SVBUWWS SVBUWSWS
#define SVHUWWS SVHUWSWS
#define SVWUWWS SVWUWSWS
#define SVBUBB SVBUBSBS
#define SVBUB SVBUBSBS
#define SVHUBB SVHUBSBS
#define SVHUB SVHUBSBS
#define SVWUBB SVWUBSBS
#define SVWUB SVWUBSBS
#define SVBUHH SVBUHSHS
#define SVBUH SVBUHSHS
#define SVHUHH SVHUHSHS
#define SVHUH SVHUHSHS
#define SVWUHH SVWUHSHS
#define SVWUH SVWUHSHS
#define SVBUWW SVBUWSWS
#define SVBUW SVBUWSWS
#define SVHUWW SVHUWSWS
#define SVHUW SVHUWSWS
#define SVWUWW SVWUWSWS
#define SVWUW SVWUWSWS
#define SVBBSBS SVBSBSBS
#define SVBBS SVBSBSBS
#define SVHBSBS SVHSBSBS
#define SVHBS SVHSBSBS
#define SVWBSBS SVWSBSBS
#define SVWBS SVWSBSBS
#define SVBHSHS SVBSHSHS
#define SVBHS SVBSHSHS
#define SVHHSHS SVHSHSHS
#define SVHHS SVHSHSHS
#define SVWHSHS SVWSHSHS
#define SVWHS SVWSHSHS
#define SVBWSWS SVBSWSWS
#define SVBWS SVBSWSWS
#define SVHWSWS SVHSWSWS
#define SVHWS SVHSWSWS
#define SVWWSWS SVWSWSWS
#define SVWWS SVWSWSWS
#define SVBBSB SVBSBSBS
#define SVHBSB SVHSBSBS
#define SVWBSB SVWSBSBS
#define SVBHSH SVBSHSHS
#define SVHHSH SVHSHSHS
#define SVWHSH SVWSHSHS
#define SVBWSW SVBSWSWS
#define SVHWSW SVHSWSWS
#define SVWWSW SVWSWSWS
#define SVBBUBU SVBSBUBU
#define SVBBU SVBSBUBU
#define SVHBUBU SVHSBUBU
#define SVHBU SVHSBUBU
#define SVWBUBU SVWSBUBU
#define SVWBU SVWSBUBU
#define SVBHUHU SVBSHUHU
#define SVBHU SVBSHUHU
#define SVHHUHU SVHSHUHU
#define SVHHU SVHSHUHU
#define SVWHUHU SVWSHUHU
#define SVWHU SVWSHUHU
#define SVBWUWU SVBSWUWU
#define SVBWU SVBSWUWU
#define SVHWUWU SVHSWUWU
#define SVHWU SVHSWUWU
#define SVWWUWU SVWSWUWU
#define SVWWU SVWSWUWU
#define SVBBBS SVBSBSBS
#define SVHBBS SVHSBSBS
#define SVWBBS SVWSBSBS
#define SVBHHS SVBSHSHS
#define SVHHHS SVHSHSHS
#define SVWHHS SVWSHSHS
#define SVBWWS SVBSWSWS
#define SVHWWS SVHSWSWS
#define SVWWWS SVWSWSWS
#define SVBBB SVBSBSBS
#define SVBB SVBSBSBS
#define SVB SVBSBSBS
#define SVHBB SVHSBSBS
#define SVHB SVHSBSBS
#define SVWBB SVWSBSBS
#define SVWB SVWSBSBS
#define SVBHH SVBSHSHS
#define SVBH SVBSHSHS
#define SVHHH SVHSHSHS
#define SVHH SVHSHSHS
#define SVH SVHSHSHS
#define SVWHH SVWSHSHS
#define SVWH SVWSHSHS
#define SVBWW SVBSWSWS
#define SVBW SVBSWSWS
#define SVHWW SVHSWSWS
#define SVHW SVHSWSWS
#define SVWWW SVWSWSWS
#define SVWW SVWSWSWS
#define SVW SVWSWSWS
#define ENUM_CONV_VSBSBSBS(srcb) (srcb)
#define VSBSBS VSBSBSBS
#define VSBS VSBSBSBS
#define ENUM_CONV_VSHSBSBS(srcb) (srcb)
#define VSHSBS VSHSBSBS
#define ENUM_CONV_VSWSBSBS(srcb) (srcb)
#define VSWSBS VSWSBSBS
#define ENUM_CONV_VSBSBSHS(srcb) (srcb)
#define ENUM_CONV_VSHSBSHS(srcb) (srcb)
#define ENUM_CONV_VSWSBSHS(srcb) (srcb)
#define ENUM_CONV_VSBSBSWS(srcb) (srcb)
#define ENUM_CONV_VSHSBSWS(srcb) (srcb)
#define ENUM_CONV_VSWSBSWS(srcb) (srcb)
#define ENUM_CONV_VSBSHSBS(srcb) (srcb)
#define ENUM_CONV_VSHSHSBS(srcb) (srcb)
#define ENUM_CONV_VSWSHSBS(srcb) (srcb)
#define ENUM_CONV_VSBSHSHS(srcb) (srcb)
#define VSBSHS VSBSHSHS
#define ENUM_CONV_VSHSHSHS(srcb) (srcb)
#define VSHSHS VSHSHSHS
#define VSHS VSHSHSHS
#define ENUM_CONV_VSWSHSHS(srcb) (srcb)
#define VSWSHS VSWSHSHS
#define ENUM_CONV_VSBSHSWS(srcb) (srcb)
#define ENUM_CONV_VSHSHSWS(srcb) (srcb)
#define ENUM_CONV_VSWSHSWS(srcb) (srcb)
#define ENUM_CONV_VSBSWSBS(srcb) (srcb)
#define ENUM_CONV_VSHSWSBS(srcb) (srcb)
#define ENUM_CONV_VSWSWSBS(srcb) (srcb)
#define ENUM_CONV_VSBSWSHS(srcb) (srcb)
#define ENUM_CONV_VSHSWSHS(srcb) (srcb)
#define ENUM_CONV_VSWSWSHS(srcb) (srcb)
#define ENUM_CONV_VSBSWSWS(srcb) (srcb)
#define VSBSWS VSBSWSWS
#define ENUM_CONV_VSHSWSWS(srcb) (srcb)
#define VSHSWS VSHSWSWS
#define ENUM_CONV_VSWSWSWS(srcb) (srcb)
#define VSWSWS VSWSWSWS
#define VSWS VSWSWSWS
#define ENUM_CONV_VSBSBSBU(srcb) (srcb)
#define ENUM_CONV_VSHSBSBU(srcb) (srcb)
#define ENUM_CONV_VSWSBSBU(srcb) (srcb)
#define ENUM_CONV_VSBSBSHU(srcb) (srcb)
#define ENUM_CONV_VSHSBSHU(srcb) (srcb)
#define ENUM_CONV_VSWSBSHU(srcb) (srcb)
#define ENUM_CONV_VSBSBSWU(srcb) (srcb)
#define ENUM_CONV_VSHSBSWU(srcb) (srcb)
#define ENUM_CONV_VSWSBSWU(srcb) (srcb)
#define ENUM_CONV_VSBSHSBU(srcb) (srcb)
#define ENUM_CONV_VSHSHSBU(srcb) (srcb)
#define ENUM_CONV_VSWSHSBU(srcb) (srcb)
#define ENUM_CONV_VSBSHSHU(srcb) (srcb)
#define ENUM_CONV_VSHSHSHU(srcb) (srcb)
#define ENUM_CONV_VSWSHSHU(srcb) (srcb)
#define ENUM_CONV_VSBSHSWU(srcb) (srcb)
#define ENUM_CONV_VSHSHSWU(srcb) (srcb)
#define ENUM_CONV_VSWSHSWU(srcb) (srcb)
#define ENUM_CONV_VSBSWSBU(srcb) (srcb)
#define ENUM_CONV_VSHSWSBU(srcb) (srcb)
#define ENUM_CONV_VSWSWSBU(srcb) (srcb)
#define ENUM_CONV_VSBSWSHU(srcb) (srcb)
#define ENUM_CONV_VSHSWSHU(srcb) (srcb)
#define ENUM_CONV_VSWSWSHU(srcb) (srcb)
#define ENUM_CONV_VSBSWSWU(srcb) (srcb)
#define ENUM_CONV_VSHSWSWU(srcb) (srcb)
#define ENUM_CONV_VSWSWSWU(srcb) (srcb)
#define VSBSBSB VSBSBSBS
#define VSHSBSB VSHSBSBS
#define VSWSBSB VSWSBSBS
#define VSBSBSH VSBSBSHS
#define VSHSBSH VSHSBSHS
#define VSWSBSH VSWSBSHS
#define VSBSBSW VSBSBSWS
#define VSHSBSW VSHSBSWS
#define VSWSBSW VSWSBSWS
#define VSBSHSB VSBSHSBS
#define VSHSHSB VSHSHSBS
#define VSWSHSB VSWSHSBS
#define VSBSHSH VSBSHSHS
#define VSHSHSH VSHSHSHS
#define VSWSHSH VSWSHSHS
#define VSBSHSW VSBSHSWS
#define VSHSHSW VSHSHSWS
#define VSWSHSW VSWSHSWS
#define VSBSWSB VSBSWSBS
#define VSHSWSB VSHSWSBS
#define VSWSWSB VSWSWSBS
#define VSBSWSH VSBSWSHS
#define VSHSWSH VSHSWSHS
#define VSWSWSH VSWSWSHS
#define VSBSWSW VSBSWSWS
#define VSHSWSW VSHSWSWS
#define VSWSWSW VSWSWSWS
#define ENUM_CONV_VSBSBUBS(srcb) (srcb)
#define ENUM_CONV_VSHSBUBS(srcb) (srcb)
#define ENUM_CONV_VSWSBUBS(srcb) (srcb)
#define ENUM_CONV_VSBSBUHS(srcb) (srcb)
#define ENUM_CONV_VSHSBUHS(srcb) (srcb)
#define ENUM_CONV_VSWSBUHS(srcb) (srcb)
#define ENUM_CONV_VSBSBUWS(srcb) (srcb)
#define ENUM_CONV_VSHSBUWS(srcb) (srcb)
#define ENUM_CONV_VSWSBUWS(srcb) (srcb)
#define ENUM_CONV_VSBSHUBS(srcb) (srcb)
#define ENUM_CONV_VSHSHUBS(srcb) (srcb)
#define ENUM_CONV_VSWSHUBS(srcb) (srcb)
#define ENUM_CONV_VSBSHUHS(srcb) (srcb)
#define ENUM_CONV_VSHSHUHS(srcb) (srcb)
#define ENUM_CONV_VSWSHUHS(srcb) (srcb)
#define ENUM_CONV_VSBSHUWS(srcb) (srcb)
#define ENUM_CONV_VSHSHUWS(srcb) (srcb)
#define ENUM_CONV_VSWSHUWS(srcb) (srcb)
#define ENUM_CONV_VSBSWUBS(srcb) (srcb)
#define ENUM_CONV_VSHSWUBS(srcb) (srcb)
#define ENUM_CONV_VSWSWUBS(srcb) (srcb)
#define ENUM_CONV_VSBSWUHS(srcb) (srcb)
#define ENUM_CONV_VSHSWUHS(srcb) (srcb)
#define ENUM_CONV_VSWSWUHS(srcb) (srcb)
#define ENUM_CONV_VSBSWUWS(srcb) (srcb)
#define ENUM_CONV_VSHSWUWS(srcb) (srcb)
#define ENUM_CONV_VSWSWUWS(srcb) (srcb)
#define ENUM_CONV_VSBSBUBU(srcb) (srcb)
#define VSBSBU VSBSBUBU
#define ENUM_CONV_VSHSBUBU(srcb) (srcb)
#define VSHSBU VSHSBUBU
#define ENUM_CONV_VSWSBUBU(srcb) (srcb)
#define VSWSBU VSWSBUBU
#define ENUM_CONV_VSBSBUHU(srcb) (srcb)
#define ENUM_CONV_VSHSBUHU(srcb) (srcb)
#define ENUM_CONV_VSWSBUHU(srcb) (srcb)
#define ENUM_CONV_VSBSBUWU(srcb) (srcb)
#define ENUM_CONV_VSHSBUWU(srcb) (srcb)
#define ENUM_CONV_VSWSBUWU(srcb) (srcb)
#define ENUM_CONV_VSBSHUBU(srcb) (srcb)
#define ENUM_CONV_VSHSHUBU(srcb) (srcb)
#define ENUM_CONV_VSWSHUBU(srcb) (srcb)
#define ENUM_CONV_VSBSHUHU(srcb) (srcb)
#define VSBSHU VSBSHUHU
#define ENUM_CONV_VSHSHUHU(srcb) (srcb)
#define VSHSHU VSHSHUHU
#define ENUM_CONV_VSWSHUHU(srcb) (srcb)
#define VSWSHU VSWSHUHU
#define ENUM_CONV_VSBSHUWU(srcb) (srcb)
#define ENUM_CONV_VSHSHUWU(srcb) (srcb)
#define ENUM_CONV_VSWSHUWU(srcb) (srcb)
#define ENUM_CONV_VSBSWUBU(srcb) (srcb)
#define ENUM_CONV_VSHSWUBU(srcb) (srcb)
#define ENUM_CONV_VSWSWUBU(srcb) (srcb)
#define ENUM_CONV_VSBSWUHU(srcb) (srcb)
#define ENUM_CONV_VSHSWUHU(srcb) (srcb)
#define ENUM_CONV_VSWSWUHU(srcb) (srcb)
#define ENUM_CONV_VSBSWUWU(srcb) (srcb)
#define VSBSWU VSBSWUWU
#define ENUM_CONV_VSHSWUWU(srcb) (srcb)
#define VSHSWU VSHSWUWU
#define ENUM_CONV_VSWSWUWU(srcb) (srcb)
#define VSWSWU VSWSWUWU
#define VSBSBUB VSBSBUBS
#define VSHSBUB VSHSBUBS
#define VSWSBUB VSWSBUBS
#define VSBSBUH VSBSBUHS
#define VSHSBUH VSHSBUHS
#define VSWSBUH VSWSBUHS
#define VSBSBUW VSBSBUWS
#define VSHSBUW VSHSBUWS
#define VSWSBUW VSWSBUWS
#define VSBSHUB VSBSHUBS
#define VSHSHUB VSHSHUBS
#define VSWSHUB VSWSHUBS
#define VSBSHUH VSBSHUHS
#define VSHSHUH VSHSHUHS
#define VSWSHUH VSWSHUHS
#define VSBSHUW VSBSHUWS
#define VSHSHUW VSHSHUWS
#define VSWSHUW VSWSHUWS
#define VSBSWUB VSBSWUBS
#define VSHSWUB VSHSWUBS
#define VSWSWUB VSWSWUBS
#define VSBSWUH VSBSWUHS
#define VSHSWUH VSHSWUHS
#define VSWSWUH VSWSWUHS
#define VSBSWUW VSBSWUWS
#define VSHSWUW VSHSWUWS
#define VSWSWUW VSWSWUWS
#define VSBSBBS VSBSBSBS
#define VSHSBBS VSHSBSBS
#define VSWSBBS VSWSBSBS
#define VSBSBHS VSBSBSHS
#define VSHSBHS VSHSBSHS
#define VSWSBHS VSWSBSHS
#define VSBSBWS VSBSBSWS
#define VSHSBWS VSHSBSWS
#define VSWSBWS VSWSBSWS
#define VSBSHBS VSBSHSBS
#define VSHSHBS VSHSHSBS
#define VSWSHBS VSWSHSBS
#define VSBSHHS VSBSHSHS
#define VSHSHHS VSHSHSHS
#define VSWSHHS VSWSHSHS
#define VSBSHWS VSBSHSWS
#define VSHSHWS VSHSHSWS
#define VSWSHWS VSWSHSWS
#define VSBSWBS VSBSWSBS
#define VSHSWBS VSHSWSBS
#define VSWSWBS VSWSWSBS
#define VSBSWHS VSBSWSHS
#define VSHSWHS VSHSWSHS
#define VSWSWHS VSWSWSHS
#define VSBSWWS VSBSWSWS
#define VSHSWWS VSHSWSWS
#define VSWSWWS VSWSWSWS
#define VSBSBBU VSBSBSBU
#define VSHSBBU VSHSBSBU
#define VSWSBBU VSWSBSBU
#define VSBSBHU VSBSBSHU
#define VSHSBHU VSHSBSHU
#define VSWSBHU VSWSBSHU
#define VSBSBWU VSBSBSWU
#define VSHSBWU VSHSBSWU
#define VSWSBWU VSWSBSWU
#define VSBSHBU VSBSHSBU
#define VSHSHBU VSHSHSBU
#define VSWSHBU VSWSHSBU
#define VSBSHHU VSBSHSHU
#define VSHSHHU VSHSHSHU
#define VSWSHHU VSWSHSHU
#define VSBSHWU VSBSHSWU
#define VSHSHWU VSHSHSWU
#define VSWSHWU VSWSHSWU
#define VSBSWBU VSBSWSBU
#define VSHSWBU VSHSWSBU
#define VSWSWBU VSWSWSBU
#define VSBSWHU VSBSWSHU
#define VSHSWHU VSHSWSHU
#define VSWSWHU VSWSWSHU
#define VSBSWWU VSBSWSWU
#define VSHSWWU VSHSWSWU
#define VSWSWWU VSWSWSWU
#define VSBSBB VSBSBSBS
#define VSBSB VSBSBSBS
#define VSHSBB VSHSBSBS
#define VSHSB VSHSBSBS
#define VSWSBB VSWSBSBS
#define VSWSB VSWSBSBS
#define VSBSBH VSBSBSHS
#define VSHSBH VSHSBSHS
#define VSWSBH VSWSBSHS
#define VSBSBW VSBSBSWS
#define VSHSBW VSHSBSWS
#define VSWSBW VSWSBSWS
#define VSBSHB VSBSHSBS
#define VSHSHB VSHSHSBS
#define VSWSHB VSWSHSBS
#define VSBSHH VSBSHSHS
#define VSBSH VSBSHSHS
#define VSHSHH VSHSHSHS
#define VSHSH VSHSHSHS
#define VSWSHH VSWSHSHS
#define VSWSH VSWSHSHS
#define VSBSHW VSBSHSWS
#define VSHSHW VSHSHSWS
#define VSWSHW VSWSHSWS
#define VSBSWB VSBSWSBS
#define VSHSWB VSHSWSBS
#define VSWSWB VSWSWSBS
#define VSBSWH VSBSWSHS
#define VSHSWH VSHSWSHS
#define VSWSWH VSWSWSHS
#define VSBSWW VSBSWSWS
#define VSBSW VSBSWSWS
#define VSHSWW VSHSWSWS
#define VSHSW VSHSWSWS
#define VSWSWW VSWSWSWS
#define VSWSW VSWSWSWS
#define ENUM_CONV_VSBUBSBS(srcb) (srcb)
#define VSBUBS VSBUBSBS
#define ENUM_CONV_VSHUBSBS(srcb) (srcb)
#define VSHUBS VSHUBSBS
#define ENUM_CONV_VSWUBSBS(srcb) (srcb)
#define VSWUBS VSWUBSBS
#define ENUM_CONV_VSBUBSHS(srcb) (srcb)
#define ENUM_CONV_VSHUBSHS(srcb) (srcb)
#define ENUM_CONV_VSWUBSHS(srcb) (srcb)
#define ENUM_CONV_VSBUBSWS(srcb) (srcb)
#define ENUM_CONV_VSHUBSWS(srcb) (srcb)
#define ENUM_CONV_VSWUBSWS(srcb) (srcb)
#define ENUM_CONV_VSBUHSBS(srcb) (srcb)
#define ENUM_CONV_VSHUHSBS(srcb) (srcb)
#define ENUM_CONV_VSWUHSBS(srcb) (srcb)
#define ENUM_CONV_VSBUHSHS(srcb) (srcb)
#define VSBUHS VSBUHSHS
#define ENUM_CONV_VSHUHSHS(srcb) (srcb)
#define VSHUHS VSHUHSHS
#define ENUM_CONV_VSWUHSHS(srcb) (srcb)
#define VSWUHS VSWUHSHS
#define ENUM_CONV_VSBUHSWS(srcb) (srcb)
#define ENUM_CONV_VSHUHSWS(srcb) (srcb)
#define ENUM_CONV_VSWUHSWS(srcb) (srcb)
#define ENUM_CONV_VSBUWSBS(srcb) (srcb)
#define ENUM_CONV_VSHUWSBS(srcb) (srcb)
#define ENUM_CONV_VSWUWSBS(srcb) (srcb)
#define ENUM_CONV_VSBUWSHS(srcb) (srcb)
#define ENUM_CONV_VSHUWSHS(srcb) (srcb)
#define ENUM_CONV_VSWUWSHS(srcb) (srcb)
#define ENUM_CONV_VSBUWSWS(srcb) (srcb)
#define VSBUWS VSBUWSWS
#define ENUM_CONV_VSHUWSWS(srcb) (srcb)
#define VSHUWS VSHUWSWS
#define ENUM_CONV_VSWUWSWS(srcb) (srcb)
#define VSWUWS VSWUWSWS
#define ENUM_CONV_VSBUBSBU(srcb) (srcb)
#define ENUM_CONV_VSHUBSBU(srcb) (srcb)
#define ENUM_CONV_VSWUBSBU(srcb) (srcb)
#define ENUM_CONV_VSBUBSHU(srcb) (srcb)
#define ENUM_CONV_VSHUBSHU(srcb) (srcb)
#define ENUM_CONV_VSWUBSHU(srcb) (srcb)
#define ENUM_CONV_VSBUBSWU(srcb) (srcb)
#define ENUM_CONV_VSHUBSWU(srcb) (srcb)
#define ENUM_CONV_VSWUBSWU(srcb) (srcb)
#define ENUM_CONV_VSBUHSBU(srcb) (srcb)
#define ENUM_CONV_VSHUHSBU(srcb) (srcb)
#define ENUM_CONV_VSWUHSBU(srcb) (srcb)
#define ENUM_CONV_VSBUHSHU(srcb) (srcb)
#define ENUM_CONV_VSHUHSHU(srcb) (srcb)
#define ENUM_CONV_VSWUHSHU(srcb) (srcb)
#define ENUM_CONV_VSBUHSWU(srcb) (srcb)
#define ENUM_CONV_VSHUHSWU(srcb) (srcb)
#define ENUM_CONV_VSWUHSWU(srcb) (srcb)
#define ENUM_CONV_VSBUWSBU(srcb) (srcb)
#define ENUM_CONV_VSHUWSBU(srcb) (srcb)
#define ENUM_CONV_VSWUWSBU(srcb) (srcb)
#define ENUM_CONV_VSBUWSHU(srcb) (srcb)
#define ENUM_CONV_VSHUWSHU(srcb) (srcb)
#define ENUM_CONV_VSWUWSHU(srcb) (srcb)
#define ENUM_CONV_VSBUWSWU(srcb) (srcb)
#define ENUM_CONV_VSHUWSWU(srcb) (srcb)
#define ENUM_CONV_VSWUWSWU(srcb) (srcb)
#define VSBUBSB VSBUBSBS
#define VSHUBSB VSHUBSBS
#define VSWUBSB VSWUBSBS
#define VSBUBSH VSBUBSHS
#define VSHUBSH VSHUBSHS
#define VSWUBSH VSWUBSHS
#define VSBUBSW VSBUBSWS
#define VSHUBSW VSHUBSWS
#define VSWUBSW VSWUBSWS
#define VSBUHSB VSBUHSBS
#define VSHUHSB VSHUHSBS
#define VSWUHSB VSWUHSBS
#define VSBUHSH VSBUHSHS
#define VSHUHSH VSHUHSHS
#define VSWUHSH VSWUHSHS
#define VSBUHSW VSBUHSWS
#define VSHUHSW VSHUHSWS
#define VSWUHSW VSWUHSWS
#define VSBUWSB VSBUWSBS
#define VSHUWSB VSHUWSBS
#define VSWUWSB VSWUWSBS
#define VSBUWSH VSBUWSHS
#define VSHUWSH VSHUWSHS
#define VSWUWSH VSWUWSHS
#define VSBUWSW VSBUWSWS
#define VSHUWSW VSHUWSWS
#define VSWUWSW VSWUWSWS
#define ENUM_CONV_VSBUBUBS(srcb) (srcb)
#define ENUM_CONV_VSHUBUBS(srcb) (srcb)
#define ENUM_CONV_VSWUBUBS(srcb) (srcb)
#define ENUM_CONV_VSBUBUHS(srcb) (srcb)
#define ENUM_CONV_VSHUBUHS(srcb) (srcb)
#define ENUM_CONV_VSWUBUHS(srcb) (srcb)
#define ENUM_CONV_VSBUBUWS(srcb) (srcb)
#define ENUM_CONV_VSHUBUWS(srcb) (srcb)
#define ENUM_CONV_VSWUBUWS(srcb) (srcb)
#define ENUM_CONV_VSBUHUBS(srcb) (srcb)
#define ENUM_CONV_VSHUHUBS(srcb) (srcb)
#define ENUM_CONV_VSWUHUBS(srcb) (srcb)
#define ENUM_CONV_VSBUHUHS(srcb) (srcb)
#define ENUM_CONV_VSHUHUHS(srcb) (srcb)
#define ENUM_CONV_VSWUHUHS(srcb) (srcb)
#define ENUM_CONV_VSBUHUWS(srcb) (srcb)
#define ENUM_CONV_VSHUHUWS(srcb) (srcb)
#define ENUM_CONV_VSWUHUWS(srcb) (srcb)
#define ENUM_CONV_VSBUWUBS(srcb) (srcb)
#define ENUM_CONV_VSHUWUBS(srcb) (srcb)
#define ENUM_CONV_VSWUWUBS(srcb) (srcb)
#define ENUM_CONV_VSBUWUHS(srcb) (srcb)
#define ENUM_CONV_VSHUWUHS(srcb) (srcb)
#define ENUM_CONV_VSWUWUHS(srcb) (srcb)
#define ENUM_CONV_VSBUWUWS(srcb) (srcb)
#define ENUM_CONV_VSHUWUWS(srcb) (srcb)
#define ENUM_CONV_VSWUWUWS(srcb) (srcb)
#define ENUM_CONV_VSBUBUBU(srcb) (srcb)
#define VSBUBU VSBUBUBU
#define VSBU VSBUBUBU
#define ENUM_CONV_VSHUBUBU(srcb) (srcb)
#define VSHUBU VSHUBUBU
#define ENUM_CONV_VSWUBUBU(srcb) (srcb)
#define VSWUBU VSWUBUBU
#define ENUM_CONV_VSBUBUHU(srcb) (srcb)
#define ENUM_CONV_VSHUBUHU(srcb) (srcb)
#define ENUM_CONV_VSWUBUHU(srcb) (srcb)
#define ENUM_CONV_VSBUBUWU(srcb) (srcb)
#define ENUM_CONV_VSHUBUWU(srcb) (srcb)
#define ENUM_CONV_VSWUBUWU(srcb) (srcb)
#define ENUM_CONV_VSBUHUBU(srcb) (srcb)
#define ENUM_CONV_VSHUHUBU(srcb) (srcb)
#define ENUM_CONV_VSWUHUBU(srcb) (srcb)
#define ENUM_CONV_VSBUHUHU(srcb) (srcb)
#define VSBUHU VSBUHUHU
#define ENUM_CONV_VSHUHUHU(srcb) (srcb)
#define VSHUHU VSHUHUHU
#define VSHU VSHUHUHU
#define ENUM_CONV_VSWUHUHU(srcb) (srcb)
#define VSWUHU VSWUHUHU
#define ENUM_CONV_VSBUHUWU(srcb) (srcb)
#define ENUM_CONV_VSHUHUWU(srcb) (srcb)
#define ENUM_CONV_VSWUHUWU(srcb) (srcb)
#define ENUM_CONV_VSBUWUBU(srcb) (srcb)
#define ENUM_CONV_VSHUWUBU(srcb) (srcb)
#define ENUM_CONV_VSWUWUBU(srcb) (srcb)
#define ENUM_CONV_VSBUWUHU(srcb) (srcb)
#define ENUM_CONV_VSHUWUHU(srcb) (srcb)
#define ENUM_CONV_VSWUWUHU(srcb) (srcb)
#define ENUM_CONV_VSBUWUWU(srcb) (srcb)
#define VSBUWU VSBUWUWU
#define ENUM_CONV_VSHUWUWU(srcb) (srcb)
#define VSHUWU VSHUWUWU
#define ENUM_CONV_VSWUWUWU(srcb) (srcb)
#define VSWUWU VSWUWUWU
#define VSWU VSWUWUWU
#define VSBUBUB VSBUBUBS
#define VSHUBUB VSHUBUBS
#define VSWUBUB VSWUBUBS
#define VSBUBUH VSBUBUHS
#define VSHUBUH VSHUBUHS
#define VSWUBUH VSWUBUHS
#define VSBUBUW VSBUBUWS
#define VSHUBUW VSHUBUWS
#define VSWUBUW VSWUBUWS
#define VSBUHUB VSBUHUBS
#define VSHUHUB VSHUHUBS
#define VSWUHUB VSWUHUBS
#define VSBUHUH VSBUHUHS
#define VSHUHUH VSHUHUHS
#define VSWUHUH VSWUHUHS
#define VSBUHUW VSBUHUWS
#define VSHUHUW VSHUHUWS
#define VSWUHUW VSWUHUWS
#define VSBUWUB VSBUWUBS
#define VSHUWUB VSHUWUBS
#define VSWUWUB VSWUWUBS
#define VSBUWUH VSBUWUHS
#define VSHUWUH VSHUWUHS
#define VSWUWUH VSWUWUHS
#define VSBUWUW VSBUWUWS
#define VSHUWUW VSHUWUWS
#define VSWUWUW VSWUWUWS
#define VSBUBBS VSBUBSBS
#define VSHUBBS VSHUBSBS
#define VSWUBBS VSWUBSBS
#define VSBUBHS VSBUBSHS
#define VSHUBHS VSHUBSHS
#define VSWUBHS VSWUBSHS
#define VSBUBWS VSBUBSWS
#define VSHUBWS VSHUBSWS
#define VSWUBWS VSWUBSWS
#define VSBUHBS VSBUHSBS
#define VSHUHBS VSHUHSBS
#define VSWUHBS VSWUHSBS
#define VSBUHHS VSBUHSHS
#define VSHUHHS VSHUHSHS
#define VSWUHHS VSWUHSHS
#define VSBUHWS VSBUHSWS
#define VSHUHWS VSHUHSWS
#define VSWUHWS VSWUHSWS
#define VSBUWBS VSBUWSBS
#define VSHUWBS VSHUWSBS
#define VSWUWBS VSWUWSBS
#define VSBUWHS VSBUWSHS
#define VSHUWHS VSHUWSHS
#define VSWUWHS VSWUWSHS
#define VSBUWWS VSBUWSWS
#define VSHUWWS VSHUWSWS
#define VSWUWWS VSWUWSWS
#define VSBUBBU VSBUBSBU
#define VSHUBBU VSHUBSBU
#define VSWUBBU VSWUBSBU
#define VSBUBHU VSBUBSHU
#define VSHUBHU VSHUBSHU
#define VSWUBHU VSWUBSHU
#define VSBUBWU VSBUBSWU
#define VSHUBWU VSHUBSWU
#define VSWUBWU VSWUBSWU
#define VSBUHBU VSBUHSBU
#define VSHUHBU VSHUHSBU
#define VSWUHBU VSWUHSBU
#define VSBUHHU VSBUHSHU
#define VSHUHHU VSHUHSHU
#define VSWUHHU VSWUHSHU
#define VSBUHWU VSBUHSWU
#define VSHUHWU VSHUHSWU
#define VSWUHWU VSWUHSWU
#define VSBUWBU VSBUWSBU
#define VSHUWBU VSHUWSBU
#define VSWUWBU VSWUWSBU
#define VSBUWHU VSBUWSHU
#define VSHUWHU VSHUWSHU
#define VSWUWHU VSWUWSHU
#define VSBUWWU VSBUWSWU
#define VSHUWWU VSHUWSWU
#define VSWUWWU VSWUWSWU
#define VSBUBB VSBUBSBS
#define VSBUB VSBUBSBS
#define VSHUBB VSHUBSBS
#define VSHUB VSHUBSBS
#define VSWUBB VSWUBSBS
#define VSWUB VSWUBSBS
#define VSBUBH VSBUBSHS
#define VSHUBH VSHUBSHS
#define VSWUBH VSWUBSHS
#define VSBUBW VSBUBSWS
#define VSHUBW VSHUBSWS
#define VSWUBW VSWUBSWS
#define VSBUHB VSBUHSBS
#define VSHUHB VSHUHSBS
#define VSWUHB VSWUHSBS
#define VSBUHH VSBUHSHS
#define VSBUH VSBUHSHS
#define VSHUHH VSHUHSHS
#define VSHUH VSHUHSHS
#define VSWUHH VSWUHSHS
#define VSWUH VSWUHSHS
#define VSBUHW VSBUHSWS
#define VSHUHW VSHUHSWS
#define VSWUHW VSWUHSWS
#define VSBUWB VSBUWSBS
#define VSHUWB VSHUWSBS
#define VSWUWB VSWUWSBS
#define VSBUWH VSBUWSHS
#define VSHUWH VSHUWSHS
#define VSWUWH VSWUWSHS
#define VSBUWW VSBUWSWS
#define VSBUW VSBUWSWS
#define VSHUWW VSHUWSWS
#define VSHUW VSHUWSWS
#define VSWUWW VSWUWSWS
#define VSWUW VSWUWSWS
#define VSBBSBS VSBSBSBS
#define VSBBS VSBSBSBS
#define VSHBSBS VSHSBSBS
#define VSHBS VSHSBSBS
#define VSWBSBS VSWSBSBS
#define VSWBS VSWSBSBS
#define VSBBSHS VSBSBSHS
#define VSHBSHS VSHSBSHS
#define VSWBSHS VSWSBSHS
#define VSBBSWS VSBSBSWS
#define VSHBSWS VSHSBSWS
#define VSWBSWS VSWSBSWS
#define VSBHSBS VSBSHSBS
#define VSHHSBS VSHSHSBS
#define VSWHSBS VSWSHSBS
#define VSBHSHS VSBSHSHS
#define VSBHS VSBSHSHS
#define VSHHSHS VSHSHSHS
#define VSHHS VSHSHSHS
#define VSWHSHS VSWSHSHS
#define VSWHS VSWSHSHS
#define VSBHSWS VSBSHSWS
#define VSHHSWS VSHSHSWS
#define VSWHSWS VSWSHSWS
#define VSBWSBS VSBSWSBS
#define VSHWSBS VSHSWSBS
#define VSWWSBS VSWSWSBS
#define VSBWSHS VSBSWSHS
#define VSHWSHS VSHSWSHS
#define VSWWSHS VSWSWSHS
#define VSBWSWS VSBSWSWS
#define VSBWS VSBSWSWS
#define VSHWSWS VSHSWSWS
#define VSHWS VSHSWSWS
#define VSWWSWS VSWSWSWS
#define VSWWS VSWSWSWS
#define VSBBSBU VSBSBSBU
#define VSHBSBU VSHSBSBU
#define VSWBSBU VSWSBSBU
#define VSBBSHU VSBSBSHU
#define VSHBSHU VSHSBSHU
#define VSWBSHU VSWSBSHU
#define VSBBSWU VSBSBSWU
#define VSHBSWU VSHSBSWU
#define VSWBSWU VSWSBSWU
#define VSBHSBU VSBSHSBU
#define VSHHSBU VSHSHSBU
#define VSWHSBU VSWSHSBU
#define VSBHSHU VSBSHSHU
#define VSHHSHU VSHSHSHU
#define VSWHSHU VSWSHSHU
#define VSBHSWU VSBSHSWU
#define VSHHSWU VSHSHSWU
#define VSWHSWU VSWSHSWU
#define VSBWSBU VSBSWSBU
#define VSHWSBU VSHSWSBU
#define VSWWSBU VSWSWSBU
#define VSBWSHU VSBSWSHU
#define VSHWSHU VSHSWSHU
#define VSWWSHU VSWSWSHU
#define VSBWSWU VSBSWSWU
#define VSHWSWU VSHSWSWU
#define VSWWSWU VSWSWSWU
#define VSBBSB VSBSBSBS
#define VSHBSB VSHSBSBS
#define VSWBSB VSWSBSBS
#define VSBBSH VSBSBSHS
#define VSHBSH VSHSBSHS
#define VSWBSH VSWSBSHS
#define VSBBSW VSBSBSWS
#define VSHBSW VSHSBSWS
#define VSWBSW VSWSBSWS
#define VSBHSB VSBSHSBS
#define VSHHSB VSHSHSBS
#define VSWHSB VSWSHSBS
#define VSBHSH VSBSHSHS
#define VSHHSH VSHSHSHS
#define VSWHSH VSWSHSHS
#define VSBHSW VSBSHSWS
#define VSHHSW VSHSHSWS
#define VSWHSW VSWSHSWS
#define VSBWSB VSBSWSBS
#define VSHWSB VSHSWSBS
#define VSWWSB VSWSWSBS
#define VSBWSH VSBSWSHS
#define VSHWSH VSHSWSHS
#define VSWWSH VSWSWSHS
#define VSBWSW VSBSWSWS
#define VSHWSW VSHSWSWS
#define VSWWSW VSWSWSWS
#define VSBBUBS VSBSBUBS
#define VSHBUBS VSHSBUBS
#define VSWBUBS VSWSBUBS
#define VSBBUHS VSBSBUHS
#define VSHBUHS VSHSBUHS
#define VSWBUHS VSWSBUHS
#define VSBBUWS VSBSBUWS
#define VSHBUWS VSHSBUWS
#define VSWBUWS VSWSBUWS
#define VSBHUBS VSBSHUBS
#define VSHHUBS VSHSHUBS
#define VSWHUBS VSWSHUBS
#define VSBHUHS VSBSHUHS
#define VSHHUHS VSHSHUHS
#define VSWHUHS VSWSHUHS
#define VSBHUWS VSBSHUWS
#define VSHHUWS VSHSHUWS
#define VSWHUWS VSWSHUWS
#define VSBWUBS VSBSWUBS
#define VSHWUBS VSHSWUBS
#define VSWWUBS VSWSWUBS
#define VSBWUHS VSBSWUHS
#define VSHWUHS VSHSWUHS
#define VSWWUHS VSWSWUHS
#define VSBWUWS VSBSWUWS
#define VSHWUWS VSHSWUWS
#define VSWWUWS VSWSWUWS
#define VSBBUBU VSBSBUBU
#define VSBBU VSBSBUBU
#define VSHBUBU VSHSBUBU
#define VSHBU VSHSBUBU
#define VSWBUBU VSWSBUBU
#define VSWBU VSWSBUBU
#define VSBBUHU VSBSBUHU
#define VSHBUHU VSHSBUHU
#define VSWBUHU VSWSBUHU
#define VSBBUWU VSBSBUWU
#define VSHBUWU VSHSBUWU
#define VSWBUWU VSWSBUWU
#define VSBHUBU VSBSHUBU
#define VSHHUBU VSHSHUBU
#define VSWHUBU VSWSHUBU
#define VSBHUHU VSBSHUHU
#define VSBHU VSBSHUHU
#define VSHHUHU VSHSHUHU
#define VSHHU VSHSHUHU
#define VSWHUHU VSWSHUHU
#define VSWHU VSWSHUHU
#define VSBHUWU VSBSHUWU
#define VSHHUWU VSHSHUWU
#define VSWHUWU VSWSHUWU
#define VSBWUBU VSBSWUBU
#define VSHWUBU VSHSWUBU
#define VSWWUBU VSWSWUBU
#define VSBWUHU VSBSWUHU
#define VSHWUHU VSHSWUHU
#define VSWWUHU VSWSWUHU
#define VSBWUWU VSBSWUWU
#define VSBWU VSBSWUWU
#define VSHWUWU VSHSWUWU
#define VSHWU VSHSWUWU
#define VSWWUWU VSWSWUWU
#define VSWWU VSWSWUWU
#define VSBBUB VSBSBUBS
#define VSHBUB VSHSBUBS
#define VSWBUB VSWSBUBS
#define VSBBUH VSBSBUHS
#define VSHBUH VSHSBUHS
#define VSWBUH VSWSBUHS
#define VSBBUW VSBSBUWS
#define VSHBUW VSHSBUWS
#define VSWBUW VSWSBUWS
#define VSBHUB VSBSHUBS
#define VSHHUB VSHSHUBS
#define VSWHUB VSWSHUBS
#define VSBHUH VSBSHUHS
#define VSHHUH VSHSHUHS
#define VSWHUH VSWSHUHS
#define VSBHUW VSBSHUWS
#define VSHHUW VSHSHUWS
#define VSWHUW VSWSHUWS
#define VSBWUB VSBSWUBS
#define VSHWUB VSHSWUBS
#define VSWWUB VSWSWUBS
#define VSBWUH VSBSWUHS
#define VSHWUH VSHSWUHS
#define VSWWUH VSWSWUHS
#define VSBWUW VSBSWUWS
#define VSHWUW VSHSWUWS
#define VSWWUW VSWSWUWS
#define VSBBBS VSBSBSBS
#define VSHBBS VSHSBSBS
#define VSWBBS VSWSBSBS
#define VSBBHS VSBSBSHS
#define VSHBHS VSHSBSHS
#define VSWBHS VSWSBSHS
#define VSBBWS VSBSBSWS
#define VSHBWS VSHSBSWS
#define VSWBWS VSWSBSWS
#define VSBHBS VSBSHSBS
#define VSHHBS VSHSHSBS
#define VSWHBS VSWSHSBS
#define VSBHHS VSBSHSHS
#define VSHHHS VSHSHSHS
#define VSWHHS VSWSHSHS
#define VSBHWS VSBSHSWS
#define VSHHWS VSHSHSWS
#define VSWHWS VSWSHSWS
#define VSBWBS VSBSWSBS
#define VSHWBS VSHSWSBS
#define VSWWBS VSWSWSBS
#define VSBWHS VSBSWSHS
#define VSHWHS VSHSWSHS
#define VSWWHS VSWSWSHS
#define VSBWWS VSBSWSWS
#define VSHWWS VSHSWSWS
#define VSWWWS VSWSWSWS
#define VSBBBU VSBSBSBU
#define VSHBBU VSHSBSBU
#define VSWBBU VSWSBSBU
#define VSBBHU VSBSBSHU
#define VSHBHU VSHSBSHU
#define VSWBHU VSWSBSHU
#define VSBBWU VSBSBSWU
#define VSHBWU VSHSBSWU
#define VSWBWU VSWSBSWU
#define VSBHBU VSBSHSBU
#define VSHHBU VSHSHSBU
#define VSWHBU VSWSHSBU
#define VSBHHU VSBSHSHU
#define VSHHHU VSHSHSHU
#define VSWHHU VSWSHSHU
#define VSBHWU VSBSHSWU
#define VSHHWU VSHSHSWU
#define VSWHWU VSWSHSWU
#define VSBWBU VSBSWSBU
#define VSHWBU VSHSWSBU
#define VSWWBU VSWSWSBU
#define VSBWHU VSBSWSHU
#define VSHWHU VSHSWSHU
#define VSWWHU VSWSWSHU
#define VSBWWU VSBSWSWU
#define VSHWWU VSHSWSWU
#define VSWWWU VSWSWSWU
#define VSBBB VSBSBSBS
#define VSBB VSBSBSBS
#define VSB VSBSBSBS
#define VSHBB VSHSBSBS
#define VSHB VSHSBSBS
#define VSWBB VSWSBSBS
#define VSWB VSWSBSBS
#define VSBBH VSBSBSHS
#define VSHBH VSHSBSHS
#define VSWBH VSWSBSHS
#define VSBBW VSBSBSWS
#define VSHBW VSHSBSWS
#define VSWBW VSWSBSWS
#define VSBHB VSBSHSBS
#define VSHHB VSHSHSBS
#define VSWHB VSWSHSBS
#define VSBHH VSBSHSHS
#define VSBH VSBSHSHS
#define VSHHH VSHSHSHS
#define VSHH VSHSHSHS
#define VSH VSHSHSHS
#define VSWHH VSWSHSHS
#define VSWH VSWSHSHS
#define VSBHW VSBSHSWS
#define VSHHW VSHSHSWS
#define VSWHW VSWSHSWS
#define VSBWB VSBSWSBS
#define VSHWB VSHSWSBS
#define VSWWB VSWSWSBS
#define VSBWH VSBSWSHS
#define VSHWH VSHSWSHS
#define VSWWH VSWSWSHS
#define VSBWW VSBSWSWS
#define VSBW VSBSWSWS
#define VSHWW VSHSWSWS
#define VSHW VSHSWSWS
#define VSWWW VSWSWSWS
#define VSWW VSWSWSWS
#define VSW VSWSWSWS
#define ENUM_CONV_VEBSBSBS(srcb) (0)
#define VEBSBS VEBSBSBS
#define VEBS VEBSBSBS
#define ENUM_CONV_VEHSBSBS(srcb) (0)
#define VEHSBS VEHSBSBS
#define ENUM_CONV_VEWSBSBS(srcb) (0)
#define VEWSBS VEWSBSBS
#define ENUM_CONV_VEBSHSHS(srcb) (0)
#define VEBSHS VEBSHSHS
#define ENUM_CONV_VEHSHSHS(srcb) (0)
#define VEHSHS VEHSHSHS
#define VEHS VEHSHSHS
#define ENUM_CONV_VEWSHSHS(srcb) (0)
#define VEWSHS VEWSHSHS
#define ENUM_CONV_VEBSWSWS(srcb) (0)
#define VEBSWS VEBSWSWS
#define ENUM_CONV_VEHSWSWS(srcb) (0)
#define VEHSWS VEHSWSWS
#define ENUM_CONV_VEWSWSWS(srcb) (0)
#define VEWSWS VEWSWSWS
#define VEWS VEWSWSWS
#define VEBSBSB VEBSBSBS
#define VEHSBSB VEHSBSBS
#define VEWSBSB VEWSBSBS
#define VEBSHSH VEBSHSHS
#define VEHSHSH VEHSHSHS
#define VEWSHSH VEWSHSHS
#define VEBSWSW VEBSWSWS
#define VEHSWSW VEHSWSWS
#define VEWSWSW VEWSWSWS
#define ENUM_CONV_VEBSBUBU(srcb) (0)
#define VEBSBU VEBSBUBU
#define ENUM_CONV_VEHSBUBU(srcb) (0)
#define VEHSBU VEHSBUBU
#define ENUM_CONV_VEWSBUBU(srcb) (0)
#define VEWSBU VEWSBUBU
#define ENUM_CONV_VEBSHUHU(srcb) (0)
#define VEBSHU VEBSHUHU
#define ENUM_CONV_VEHSHUHU(srcb) (0)
#define VEHSHU VEHSHUHU
#define ENUM_CONV_VEWSHUHU(srcb) (0)
#define VEWSHU VEWSHUHU
#define ENUM_CONV_VEBSWUWU(srcb) (0)
#define VEBSWU VEBSWUWU
#define ENUM_CONV_VEHSWUWU(srcb) (0)
#define VEHSWU VEHSWUWU
#define ENUM_CONV_VEWSWUWU(srcb) (0)
#define VEWSWU VEWSWUWU
#define VEBSBBS VEBSBSBS
#define VEHSBBS VEHSBSBS
#define VEWSBBS VEWSBSBS
#define VEBSHHS VEBSHSHS
#define VEHSHHS VEHSHSHS
#define VEWSHHS VEWSHSHS
#define VEBSWWS VEBSWSWS
#define VEHSWWS VEHSWSWS
#define VEWSWWS VEWSWSWS
#define VEBSBB VEBSBSBS
#define VEBSB VEBSBSBS
#define VEHSBB VEHSBSBS
#define VEHSB VEHSBSBS
#define VEWSBB VEWSBSBS
#define VEWSB VEWSBSBS
#define VEBSHH VEBSHSHS
#define VEBSH VEBSHSHS
#define VEHSHH VEHSHSHS
#define VEHSH VEHSHSHS
#define VEWSHH VEWSHSHS
#define VEWSH VEWSHSHS
#define VEBSWW VEBSWSWS
#define VEBSW VEBSWSWS
#define VEHSWW VEHSWSWS
#define VEHSW VEHSWSWS
#define VEWSWW VEWSWSWS
#define VEWSW VEWSWSWS
#define ENUM_CONV_VEBUBSBS(srcb) (0)
#define VEBUBS VEBUBSBS
#define ENUM_CONV_VEHUBSBS(srcb) (0)
#define VEHUBS VEHUBSBS
#define ENUM_CONV_VEWUBSBS(srcb) (0)
#define VEWUBS VEWUBSBS
#define ENUM_CONV_VEBUHSHS(srcb) (0)
#define VEBUHS VEBUHSHS
#define ENUM_CONV_VEHUHSHS(srcb) (0)
#define VEHUHS VEHUHSHS
#define ENUM_CONV_VEWUHSHS(srcb) (0)
#define VEWUHS VEWUHSHS
#define ENUM_CONV_VEBUWSWS(srcb) (0)
#define VEBUWS VEBUWSWS
#define ENUM_CONV_VEHUWSWS(srcb) (0)
#define VEHUWS VEHUWSWS
#define ENUM_CONV_VEWUWSWS(srcb) (0)
#define VEWUWS VEWUWSWS
#define VEBUBSB VEBUBSBS
#define VEHUBSB VEHUBSBS
#define VEWUBSB VEWUBSBS
#define VEBUHSH VEBUHSHS
#define VEHUHSH VEHUHSHS
#define VEWUHSH VEWUHSHS
#define VEBUWSW VEBUWSWS
#define VEHUWSW VEHUWSWS
#define VEWUWSW VEWUWSWS
#define ENUM_CONV_VEBUBUBU(srcb) (0)
#define VEBUBU VEBUBUBU
#define VEBU VEBUBUBU
#define ENUM_CONV_VEHUBUBU(srcb) (0)
#define VEHUBU VEHUBUBU
#define ENUM_CONV_VEWUBUBU(srcb) (0)
#define VEWUBU VEWUBUBU
#define ENUM_CONV_VEBUHUHU(srcb) (0)
#define VEBUHU VEBUHUHU
#define ENUM_CONV_VEHUHUHU(srcb) (0)
#define VEHUHU VEHUHUHU
#define VEHU VEHUHUHU
#define ENUM_CONV_VEWUHUHU(srcb) (0)
#define VEWUHU VEWUHUHU
#define ENUM_CONV_VEBUWUWU(srcb) (0)
#define VEBUWU VEBUWUWU
#define ENUM_CONV_VEHUWUWU(srcb) (0)
#define VEHUWU VEHUWUWU
#define ENUM_CONV_VEWUWUWU(srcb) (0)
#define VEWUWU VEWUWUWU
#define VEWU VEWUWUWU
#define VEBUBBS VEBUBSBS
#define VEHUBBS VEHUBSBS
#define VEWUBBS VEWUBSBS
#define VEBUHHS VEBUHSHS
#define VEHUHHS VEHUHSHS
#define VEWUHHS VEWUHSHS
#define VEBUWWS VEBUWSWS
#define VEHUWWS VEHUWSWS
#define VEWUWWS VEWUWSWS
#define VEBUBB VEBUBSBS
#define VEBUB VEBUBSBS
#define VEHUBB VEHUBSBS
#define VEHUB VEHUBSBS
#define VEWUBB VEWUBSBS
#define VEWUB VEWUBSBS
#define VEBUHH VEBUHSHS
#define VEBUH VEBUHSHS
#define VEHUHH VEHUHSHS
#define VEHUH VEHUHSHS
#define VEWUHH VEWUHSHS
#define VEWUH VEWUHSHS
#define VEBUWW VEBUWSWS
#define VEBUW VEBUWSWS
#define VEHUWW VEHUWSWS
#define VEHUW VEHUWSWS
#define VEWUWW VEWUWSWS
#define VEWUW VEWUWSWS
#define VEBBSBS VEBSBSBS
#define VEBBS VEBSBSBS
#define VEHBSBS VEHSBSBS
#define VEHBS VEHSBSBS
#define VEWBSBS VEWSBSBS
#define VEWBS VEWSBSBS
#define VEBHSHS VEBSHSHS
#define VEBHS VEBSHSHS
#define VEHHSHS VEHSHSHS
#define VEHHS VEHSHSHS
#define VEWHSHS VEWSHSHS
#define VEWHS VEWSHSHS
#define VEBWSWS VEBSWSWS
#define VEBWS VEBSWSWS
#define VEHWSWS VEHSWSWS
#define VEHWS VEHSWSWS
#define VEWWSWS VEWSWSWS
#define VEWWS VEWSWSWS
#define VEBBSB VEBSBSBS
#define VEHBSB VEHSBSBS
#define VEWBSB VEWSBSBS
#define VEBHSH VEBSHSHS
#define VEHHSH VEHSHSHS
#define VEWHSH VEWSHSHS
#define VEBWSW VEBSWSWS
#define VEHWSW VEHSWSWS
#define VEWWSW VEWSWSWS
#define VEBBUBU VEBSBUBU
#define VEBBU VEBSBUBU
#define VEHBUBU VEHSBUBU
#define VEHBU VEHSBUBU
#define VEWBUBU VEWSBUBU
#define VEWBU VEWSBUBU
#define VEBHUHU VEBSHUHU
#define VEBHU VEBSHUHU
#define VEHHUHU VEHSHUHU
#define VEHHU VEHSHUHU
#define VEWHUHU VEWSHUHU
#define VEWHU VEWSHUHU
#define VEBWUWU VEBSWUWU
#define VEBWU VEBSWUWU
#define VEHWUWU VEHSWUWU
#define VEHWU VEHSWUWU
#define VEWWUWU VEWSWUWU
#define VEWWU VEWSWUWU
#define VEBBBS VEBSBSBS
#define VEHBBS VEHSBSBS
#define VEWBBS VEWSBSBS
#define VEBHHS VEBSHSHS
#define VEHHHS VEHSHSHS
#define VEWHHS VEWSHSHS
#define VEBWWS VEBSWSWS
#define VEHWWS VEHSWSWS
#define VEWWWS VEWSWSWS
#define VEBBB VEBSBSBS
#define VEBB VEBSBSBS
#define VEB VEBSBSBS
#define VEHBB VEHSBSBS
#define VEHB VEHSBSBS
#define VEWBB VEWSBSBS
#define VEWB VEWSBSBS
#define VEBHH VEBSHSHS
#define VEBH VEBSHSHS
#define VEHHH VEHSHSHS
#define VEHH VEHSHSHS
#define VEH VEHSHSHS
#define VEWHH VEWSHSHS
#define VEWH VEWSHSHS
#define VEBWW VEBSWSWS
#define VEBW VEBSWSWS
#define VEHWW VEHSWSWS
#define VEHW VEHSWSWS
#define VEWWW VEWSWSWS
#define VEWW VEWSWSWS
#define VEW VEWSWSWS
#define ENUM_CONV_SEBSBSBS(srcb) (0)
#define SEBSBS SEBSBSBS
#define SEBS SEBSBSBS
#define ENUM_CONV_SEHSHSHS(srcb) (0)
#define SEHSHS SEHSHSHS
#define SEHS SEHSHSHS
#define ENUM_CONV_SEWSWSWS(srcb) (0)
#define SEWSWS SEWSWSWS
#define SEWS SEWSWSWS
#define SEBSBSB SEBSBSBS
#define SEHSHSH SEHSHSHS
#define SEWSWSW SEWSWSWS
#define SEBSBBS SEBSBSBS
#define SEHSHHS SEHSHSHS
#define SEWSWWS SEWSWSWS
#define SEBSBB SEBSBSBS
#define SEBSB SEBSBSBS
#define SEHSHH SEHSHSHS
#define SEHSH SEHSHSHS
#define SEWSWW SEWSWSWS
#define SEWSW SEWSWSWS
#define ENUM_CONV_SEBUBUBU(srcb) (0)
#define SEBUBU SEBUBUBU
#define SEBU SEBUBUBU
#define ENUM_CONV_SEHUHUHU(srcb) (0)
#define SEHUHU SEHUHUHU
#define SEHU SEHUHUHU
#define ENUM_CONV_SEWUWUWU(srcb) (0)
#define SEWUWU SEWUWUWU
#define SEWU SEWUWUWU
#define SEBBSBS SEBSBSBS
#define SEBBS SEBSBSBS
#define SEHHSHS SEHSHSHS
#define SEHHS SEHSHSHS
#define SEWWSWS SEWSWSWS
#define SEWWS SEWSWSWS
#define SEBBSB SEBSBSBS
#define SEHHSH SEHSHSHS
#define SEWWSW SEWSWSWS
#define SEBBBS SEBSBSBS
#define SEHHHS SEHSHSHS
#define SEWWWS SEWSWSWS
#define SEBBB SEBSBSBS
#define SEBB SEBSBSBS
#define SEB SEBSBSBS
#define SEHHH SEHSHSHS
#define SEHH SEHSHSHS
#define SEH SEHSHSHS
#define SEWWW SEWSWSWS
#define SEWW SEWSWSWS
#define SEW SEWSWSWS

__attribute__((always_inline)) static inline void vbx_VVBSBSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBSBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBSHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBSWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBSWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSBUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUBUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBSWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBSWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSBUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUBUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBSWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBSWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSBUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUBUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUBUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUBUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUBUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUBUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUBUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUBUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUBUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUBUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUBUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUBUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUBUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUBUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUBUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUBUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUBUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUBUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUBUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUBUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUBUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUBUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUBUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUBUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUBUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUBUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUBUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUBUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUBUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUBUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUBUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUBUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUBUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUBUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUBUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUBUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUBUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUBUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUBUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUBUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUBUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUBUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHSWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHSWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSHUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUHUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHSWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHSWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSHUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUHUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHSWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHSWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSHUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUHUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUHUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUHUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUHUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUHUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUHUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUHUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUHUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUHUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUHUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUHUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUHUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUHUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUHUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUHUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUHUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUHUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUHUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUHUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUHUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUHUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUHUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUHUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUHUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUHUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUHUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUHUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUHUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUHUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUHUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUHUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUHUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUHUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUHUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUHUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUHUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUHUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUHUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUHUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUHUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUHUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUBS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUBS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUBU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUBU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUHS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUHS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUHU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUHU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWSWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWSWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBSWUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBSWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBSWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBSWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBSWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBSWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBSWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBSWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBSWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBSWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBSWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBSWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBSWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBSWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBSWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBSWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBSWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBSWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBSWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBSWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBSWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBSWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBSWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBSWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBSWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBSWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBSWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBSWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBSWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBSWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBSWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBSWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBSWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBSWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBSWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBSWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBSWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBSWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBSWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBSWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVBUWUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVBUWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVBUWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVBUWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVBUWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVBUWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVBUWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVBUWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVBUWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVBUWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVBUWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVBUWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVBUWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVBUWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVBUWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVBUWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVBUWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVBUWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVBUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVBUWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVBUWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVBUWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVBUWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVBUWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVBUWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVBUWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVBUWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVBUWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVBUWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVBUWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVBUWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVBUWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVBUWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVBUWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVBUWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVBUWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVBUWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVBUWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVBUWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVBUWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVBUWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWSWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWSWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHSWUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHSWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHSWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHSWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHSWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHSWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHSWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHSWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHSWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHSWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHSWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHSWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHSWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHSWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHSWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHSWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHSWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHSWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHSWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHSWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHSWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHSWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHSWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHSWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHSWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHSWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHSWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHSWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHSWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHSWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHSWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHSWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHSWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHSWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHSWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHSWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHSWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHSWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHSWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHSWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVHUWUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVHUWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVHUWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVHUWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVHUWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVHUWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVHUWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVHUWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVHUWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVHUWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVHUWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVHUWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVHUWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVHUWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVHUWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVHUWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVHUWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVHUWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVHUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVHUWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVHUWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVHUWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVHUWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVHUWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVHUWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVHUWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVHUWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVHUWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVHUWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVHUWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVHUWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVHUWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVHUWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVHUWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVHUWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVHUWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVHUWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVHUWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVHUWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVHUWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVHUWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWSWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWSWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWSWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWSWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWSWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWSWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWSWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWSWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWSWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWSWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWSWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWSWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWSWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWSWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWSWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWSWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWSWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWSWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWSWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWSWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWSWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWSWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWSWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWSWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWSWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWSWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWSWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWSWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWSWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWSWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWSWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWSWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWSWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWSWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWSWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWSWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWSWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWSWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWSWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWSWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWSWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWSWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUWS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUWS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWSWUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWSWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWSWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWSWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWSWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWSWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWSWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWSWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWSWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWSWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWSWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWSWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWSWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWSWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWSWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWSWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWSWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWSWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWSWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWSWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWSWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWSWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWSWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWSWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWSWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWSWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWSWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWSWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWSWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWSWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWSWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWSWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWSWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWSWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWSWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWSWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWSWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWSWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWSWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWSWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWUWUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWUWUWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWUWUWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWUWUWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWUWUWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWUWUWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWUWUWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWUWUWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWUWUWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWUWUWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWUWUWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWUWUWU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWUWUWU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWUWUWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWUWUWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWUWUWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWUWUWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWUWUWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWUWUWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWUWUWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWUWUWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWUWUWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWUWUWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWUWUWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWUWUWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWUWUWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWUWUWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWUWUWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWUWUWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWUWUWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWUWUWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWUWUWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWUWUWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWUWUWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWUWUWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWUWUWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWUWUWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWUWUWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWUWUWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWUWUWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSBSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUBSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSBUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUBUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSBSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUBSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSBUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUBUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSBSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUBSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t  s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUBSBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUBSBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUBSBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUBSBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUBSBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUBSBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUBSBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUBSBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUBSBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUBSBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUBSBS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUBSBS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUBSBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUBSBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUBSBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUBSBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUBSBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUBSBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUBSBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUBSBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUBSBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUBSBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUBSBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUBSBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUBSBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUBSBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUBSBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUBSBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUBSBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUBSBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUBSBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUBSBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUBSBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUBSBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUBSBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUBSBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUBSBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUBSBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUBSBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSBUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUBUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUBUBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUBUBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUBUBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUBUBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUBUBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUBUBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUBUBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUBUBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUBUBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUBUBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUBUBU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUBUBU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUBUBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUBUBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUBUBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUBUBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUBUBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUBUBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUBUBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUBUBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUBUBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUBUBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUBUBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUBUBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUBUBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUBUBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUBUBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUBUBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUBUBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUBUBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUBUBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUBUBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUBUBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUBUBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUBUBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUBUBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUBUBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUBUBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUBUBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSHSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUHSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSHUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUHUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSHSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUHSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSHUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUHUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSHSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUHSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t  s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUHSHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUHSHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUHSHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUHSHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUHSHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUHSHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUHSHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUHSHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUHSHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUHSHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUHSHS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUHSHS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUHSHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUHSHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUHSHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUHSHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUHSHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUHSHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUHSHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUHSHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUHSHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUHSHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUHSHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUHSHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUHSHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUHSHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUHSHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUHSHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUHSHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUHSHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUHSHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUHSHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUHSHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUHSHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUHSHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUHSHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUHSHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUHSHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUHSHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSHUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUHUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUHUHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUHUHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUHUHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUHUHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUHUHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUHUHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUHUHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUHUHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUHUHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUHUHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUHUHU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUHUHU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUHUHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUHUHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUHUHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUHUHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUHUHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUHUHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUHUHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUHUHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUHUHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUHUHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUHUHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUHUHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUHUHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUHUHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUHUHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUHUHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUHUHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUHUHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUHUHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUHUHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUHUHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUHUHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUHUHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUHUHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUHUHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUHUHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUHUHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSWSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUWSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBSWUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBSWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBSWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBSWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBSWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBSWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBSWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBSWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBSWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBSWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBSWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBSWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBSWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBSWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBSWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBSWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBSWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBSWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBSWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBSWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBSWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBSWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBSWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBSWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBSWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBSWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBSWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBSWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBSWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVBUWUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVBUWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVBUWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVBUWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVBUWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVBUWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVBUWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVBUWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVBUWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVBUWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVBUWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVBUWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVBUWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVBUWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVBUWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVBUWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVBUWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVBUWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVBUWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVBUWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVBUWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVBUWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVBUWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVBUWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVBUWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVBUWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVBUWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVBUWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVBUWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSWSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUWSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHSWUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHSWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHSWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHSWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHSWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHSWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHSWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHSWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHSWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHSWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHSWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHSWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHSWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHSWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHSWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHSWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHSWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHSWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHSWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHSWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHSWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHSWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHSWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHSWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHSWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHSWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHSWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHSWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHSWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVHUWUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVHUWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVHUWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVHUWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVHUWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVHUWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVHUWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVHUWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVHUWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVHUWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVHUWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVHUWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVHUWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVHUWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVHUWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVHUWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVHUWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVHUWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVHUWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVHUWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVHUWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVHUWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVHUWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVHUWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVHUWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVHUWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVHUWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVHUWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVHUWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSWSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUWSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUWSWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUWSWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUWSWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUWSWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUWSWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUWSWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUWSWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUWSWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUWSWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUWSWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUWSWS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUWSWS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUWSWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUWSWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUWSWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUWSWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUWSWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUWSWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUWSWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUWSWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUWSWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUWSWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUWSWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUWSWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUWSWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUWSWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUWSWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUWSWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUWSWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUWSWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUWSWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUWSWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUWSWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUWSWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUWSWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUWSWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUWSWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUWSWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUWSWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWSWUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWSWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWSWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWSWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWSWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWSWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWSWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWSWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWSWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWSWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWSWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWSWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWSWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWSWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWSWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWSWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWSWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWSWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWSWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWSWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWSWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWSWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWSWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWSWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWSWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWSWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWSWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWSWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWSWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWUWUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUWUWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWUWUWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWUWUWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUWUWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWUWUWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUWUWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUWUWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWUWUWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUWUWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUWUWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWUWUWU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWUWUWU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWUWUWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWUWUWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWUWUWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWUWUWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWUWUWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWUWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUWUWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUWUWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUWUWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUWUWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUWUWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUWUWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWUWUWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWUWUWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWUWUWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWUWUWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWUWUWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWUWUWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWUWUWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWUWUWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWUWUWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWUWUWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWUWUWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWUWUWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWUWUWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWUWUWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWUWUWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWUWUWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSBSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUBSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSBUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUBUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSBSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUBSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSBUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUBUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSBSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUBSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUBSBS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUBSBS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUBSBS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUBSBS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUBSBS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUBSBS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUBSBS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUBSBS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUBSBS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUBSBS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUBSBS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUBSBS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUBSBS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUBSBS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUBSBS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUBSBS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUBSBS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUBSBS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUBSBS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUBSBS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUBSBS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUBSBS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUBSBS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUBSBS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSBUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUBUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUBUBU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUBUBU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUBUBU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUBUBU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUBUBU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUBUBU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUBUBU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUBUBU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUBUBU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUBUBU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUBUBU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUBUBU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUBUBU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUBUBU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUBUBU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUBUBU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUBUBU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUBUBU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUBUBU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUBUBU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUBUBU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUBUBU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUBUBU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUBUBU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSHSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUHSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSHUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUHUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSHSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUHSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSHUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUHUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSHSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUHSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUHSHS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUHSHS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUHSHS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUHSHS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUHSHS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUHSHS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUHSHS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUHSHS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUHSHS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUHSHS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUHSHS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUHSHS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUHSHS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUHSHS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUHSHS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUHSHS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUHSHS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUHSHS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUHSHS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUHSHS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUHSHS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUHSHS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUHSHS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUHSHS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSHUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUHUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUHUHU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUHUHU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUHUHU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUHUHU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUHUHU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUHUHU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUHUHU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUHUHU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUHUHU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUHUHU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUHUHU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUHUHU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUHUHU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUHUHU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUHUHU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUHUHU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUHUHU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUHUHU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUHUHU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUHUHU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUHUHU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUHUHU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUHUHU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUHUHU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSWSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUWSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBSWUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBSWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBSWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBSWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBSWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBSWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBSWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBSWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBSWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBSWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBSWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBSWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBSWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBSWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBSWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBSWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBSWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBSWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBSWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBSWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBSWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBSWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBSWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBSWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSBUWUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVBUWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVBUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVBUWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVBUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVBUWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVBUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVBUWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVBUWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVBUWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVBUWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVBUWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVBUWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVBUWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVBUWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVBUWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVBUWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVBUWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVBUWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVBUWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVBUWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVBUWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVBUWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVBUWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVBUWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVBUWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVBUWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSWSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUWSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHSWUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHSWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHSWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHSWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHSWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHSWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHSWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHSWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHSWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHSWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHSWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHSWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHSWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHSWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHSWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHSWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHSWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHSWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHSWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHSWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHSWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHSWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHSWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHSWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSHUWUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVHUWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVHUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVHUWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVHUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVHUWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVHUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVHUWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVHUWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVHUWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVHUWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVHUWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVHUWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVHUWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVHUWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVHUWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVHUWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVHUWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVHUWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVHUWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVHUWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVHUWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVHUWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVHUWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVHUWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVHUWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVHUWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSWSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUWSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUWSWS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUWSWS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUWSWS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUWSWS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUWSWS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUWSWS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUWSWS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUWSWS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUWSWS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUWSWS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUWSWS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUWSWS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUWSWS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUWSWS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUWSWS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUWSWS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUWSWS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUWSWS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUWSWS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUWSWS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUWSWS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUWSWS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUWSWS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUWSWS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWSWUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWSWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWSWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWSWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWSWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWSWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWSWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWSWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWSWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWSWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWSWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWSWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWSWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWSWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWSWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWSWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWSWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWSWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWSWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWSWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWSWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWSWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWSWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWSWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWSWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWUWUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWUWUWU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWUWUWU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWUWUWU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWUWUWU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWUWUWU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWUWUWU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWUWUWU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWUWUWU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWUWUWU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWUWUWU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWUWUWU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWUWUWU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWUWUWU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWUWUWU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWUWUWU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWUWUWU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWUWUWU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWUWUWU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWUWUWU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWUWUWU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWUWUWU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWUWUWU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWUWUWU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWUWUWU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSBSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUBSBS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSBUBU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUBUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSBSBS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUBSBS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSBUBU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUBUBU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSBSBS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUBSBS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUBSBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUBSBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUBSBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUBSBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUBSBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUBSBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUBSBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUBSBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUBSBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUBSBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUBSBS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUBSBS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUBSBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUBSBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUBSBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUBSBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUBSBS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUBSBS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUBSBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUBSBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUBSBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUBSBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUBSBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUBSBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUBSBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUBSBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUBSBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUBSBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUBSBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUBSBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUBSBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUBSBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUBSBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUBSBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUBSBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUBSBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUBSBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUBSBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUBSBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUBSBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSBUBU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUBUBU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUBUBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUBUBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUBUBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUBUBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUBUBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUBUBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUBUBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUBUBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUBUBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUBUBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUBUBU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUBUBU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUBUBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUBUBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUBUBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUBUBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUBUBU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUBUBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUBUBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUBUBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUBUBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUBUBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUBUBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUBUBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUBUBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUBUBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUBUBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUBUBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUBUBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUBUBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUBUBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUBUBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUBUBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUBUBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUBUBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUBUBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUBUBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUBUBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUBUBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUBUBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSHSHS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUHSHS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSHUHU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUHUHU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSHSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUHSHS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSHUHU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUHUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSHSHS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUHSHS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUHSHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUHSHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUHSHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUHSHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUHSHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUHSHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUHSHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUHSHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUHSHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUHSHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUHSHS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUHSHS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUHSHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUHSHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUHSHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUHSHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUHSHS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUHSHS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUHSHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUHSHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUHSHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUHSHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUHSHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUHSHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUHSHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUHSHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUHSHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUHSHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUHSHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUHSHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUHSHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUHSHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUHSHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUHSHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUHSHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUHSHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUHSHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUHSHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUHSHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUHSHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSHUHU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUHUHU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUHUHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUHUHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUHUHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUHUHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUHUHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUHUHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUHUHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUHUHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUHUHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUHUHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUHUHU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUHUHU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUHUHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUHUHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUHUHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUHUHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUHUHU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUHUHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUHUHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUHUHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUHUHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUHUHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUHUHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUHUHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUHUHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUHUHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUHUHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUHUHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUHUHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUHUHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUHUHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUHUHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUHUHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUHUHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUHUHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUHUHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUHUHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUHUHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUHUHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUHUHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSWSWS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUWSWS(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBSWUWU(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBSWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBSWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBSWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBSWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBSWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBSWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBSWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBSWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBSWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBSWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBSWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBSWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBSWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBSWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBSWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBSWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBSWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBSWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBSWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBSWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBSWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBSWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBSWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBSWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBSWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBSWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBSWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBSWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBSWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBSWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBSWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBSWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBSWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBSWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBSWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBSWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBSWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBSWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBSWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEBUWUWU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEBUWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEBUWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEBUWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEBUWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEBUWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEBUWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEBUWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEBUWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEBUWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEBUWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEBUWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEBUWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEBUWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEBUWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEBUWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEBUWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEBUWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEBUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEBUWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEBUWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEBUWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEBUWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEBUWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEBUWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEBUWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEBUWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEBUWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEBUWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEBUWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEBUWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEBUWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEBUWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEBUWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEBUWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEBUWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEBUWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEBUWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEBUWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEBUWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEBUWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSWSWS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUWSWS(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHSWUWU(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHSWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHSWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHSWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHSWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHSWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHSWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHSWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHSWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHSWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHSWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHSWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHSWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHSWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHSWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHSWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHSWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHSWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHSWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHSWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHSWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHSWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHSWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHSWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHSWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHSWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHSWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHSWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHSWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHSWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHSWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHSWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHSWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHSWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHSWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHSWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHSWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHSWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHSWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHSWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEHUWUWU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEHUWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEHUWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEHUWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEHUWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEHUWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEHUWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEHUWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEHUWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEHUWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEHUWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEHUWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEHUWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEHUWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEHUWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEHUWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEHUWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEHUWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEHUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEHUWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEHUWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEHUWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEHUWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEHUWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEHUWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEHUWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEHUWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEHUWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEHUWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEHUWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEHUWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEHUWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEHUWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEHUWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEHUWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEHUWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEHUWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEHUWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEHUWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEHUWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEHUWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSWSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUWSWS(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUWSWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUWSWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUWSWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUWSWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUWSWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUWSWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUWSWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUWSWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUWSWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUWSWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUWSWS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUWSWS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUWSWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUWSWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUWSWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUWSWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUWSWS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUWSWS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUWSWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUWSWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUWSWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUWSWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUWSWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUWSWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUWSWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUWSWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUWSWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUWSWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUWSWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUWSWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUWSWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUWSWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUWSWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUWSWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUWSWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUWSWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUWSWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUWSWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUWSWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUWSWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWSWUWU(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWSWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWSWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWSWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWSWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWSWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWSWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWSWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWSWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWSWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWSWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWSWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWSWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWSWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWSWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWSWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWSWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWSWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWSWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWSWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWSWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWSWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWSWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWSWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWSWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWSWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWSWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWSWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWSWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWSWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWSWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWSWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWSWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWSWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWSWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWSWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWSWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWSWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWSWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWSWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWSWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWUWUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWUWUWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWUWUWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWUWUWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWUWUWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWUWUWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWUWUWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWUWUWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWUWUWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWUWUWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWUWUWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWUWUWU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWUWUWU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWUWUWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWUWUWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWUWUWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWUWUWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWUWUWU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWUWUWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWUWUWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWUWUWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWUWUWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWUWUWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWUWUWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWUWUWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWUWUWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWUWUWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWUWUWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWUWUWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWUWUWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWUWUWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWUWUWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWUWUWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWUWUWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWUWUWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWUWUWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWUWUWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWUWUWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWUWUWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWUWUWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWUWUWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEBSBSBS(int modify, vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEBSBSBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEBSBSBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEBSBSBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEBSBSBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEBSBSBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEBSBSBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEBSBSBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEBSBSBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEBSBSBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEBSBSBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEBSBSBS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEBSBSBS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEBSBSBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEBSBSBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEBSBSBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEBSBSBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEBSBSBS, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEBSBSBS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEBSBSBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEBSBSBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEBSBSBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEBSBSBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEBSBSBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEBSBSBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEBSBSBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEBSBSBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEBSBSBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEBSBSBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEBSBSBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEBSBSBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEBSBSBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEBSBSBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEBSBSBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEBSBSBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEBSBSBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEBSBSBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEBSBSBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEBSBSBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEBSBSBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEBSBSBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEBUBUBU(int modify, vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEBUBUBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEBUBUBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEBUBUBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEBUBUBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEBUBUBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEBUBUBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEBUBUBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEBUBUBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEBUBUBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEBUBUBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEBUBUBU, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEBUBUBU, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEBUBUBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEBUBUBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEBUBUBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEBUBUBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEBUBUBU, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEBUBUBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEBUBUBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEBUBUBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEBUBUBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEBUBUBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEBUBUBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEBUBUBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEBUBUBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEBUBUBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEBUBUBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEBUBUBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEBUBUBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEBUBUBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEBUBUBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEBUBUBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEBUBUBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEBUBUBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEBUBUBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEBUBUBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEBUBUBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEBUBUBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEBUBUBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEBUBUBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEHSHSHS(int modify, vinstr_t v_op, vbx_half_t *v_out, vbx_word_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEHSHSHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEHSHSHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEHSHSHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEHSHSHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEHSHSHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEHSHSHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEHSHSHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEHSHSHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEHSHSHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEHSHSHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEHSHSHS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEHSHSHS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEHSHSHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEHSHSHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEHSHSHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEHSHSHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEHSHSHS, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEHSHSHS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEHSHSHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEHSHSHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEHSHSHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEHSHSHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEHSHSHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEHSHSHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEHSHSHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEHSHSHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEHSHSHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEHSHSHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEHSHSHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEHSHSHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEHSHSHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEHSHSHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEHSHSHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEHSHSHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEHSHSHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEHSHSHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEHSHSHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEHSHSHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEHSHSHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEHSHSHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEHUHUHU(int modify, vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEHUHUHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEHUHUHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEHUHUHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEHUHUHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEHUHUHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEHUHUHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEHUHUHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEHUHUHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEHUHUHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEHUHUHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEHUHUHU, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEHUHUHU, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEHUHUHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEHUHUHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEHUHUHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEHUHUHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEHUHUHU, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEHUHUHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEHUHUHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEHUHUHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEHUHUHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEHUHUHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEHUHUHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEHUHUHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEHUHUHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEHUHUHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEHUHUHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEHUHUHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEHUHUHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEHUHUHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEHUHUHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEHUHUHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEHUHUHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEHUHUHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEHUHUHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEHUHUHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEHUHUHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEHUHUHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEHUHUHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEHUHUHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWSWSWS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWSWSWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWSWSWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWSWSWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWSWSWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWSWSWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWSWSWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWSWSWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWSWSWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWSWSWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWSWSWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWSWSWS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWSWSWS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWSWSWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWSWSWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWSWSWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWSWSWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWSWSWS, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWSWSWS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWSWSWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWSWSWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWSWSWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWSWSWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWSWSWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWSWSWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWSWSWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWSWSWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWSWSWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWSWSWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWSWSWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWSWSWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWSWSWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWSWSWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWSWSWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWSWSWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWSWSWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWSWSWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWSWSWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWSWSWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWSWSWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWSWSWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWUWUWU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWUWUWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWUWUWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWUWUWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWUWUWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWUWUWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWUWUWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWUWUWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWUWUWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWUWUWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWUWUWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWUWUWU, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWUWUWU, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWUWUWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWUWUWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWUWUWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWUWUWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWUWUWU, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWUWUWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWUWUWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWUWUWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWUWUWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWUWUWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWUWUWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWUWUWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWUWUWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWUWUWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWUWUWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWUWUWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWUWUWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWUWUWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWUWUWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWUWUWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWUWUWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWUWUWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWUWUWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWUWUWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWUWUWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWUWUWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWUWUWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWUWUWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}
//


#define _vbx(MODIFY,VMODE,VINSTR,DEST,SRCA,SRCB)  vbx_##VMODE(MODIFY,VINSTR,DEST,SRCA,SRCB)

#define vbx(VMODE,VINSTR,DEST,SRCA,SRCB)            _vbx( MOD_NONE, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked(VMODE,VINSTR,DEST,SRCA,SRCB)     _vbx( MOD_MASKED, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_acc(VMODE,VINSTR,DEST,SRCA,SRCB)        _vbx(MOD_ACC, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked_acc(VMODE,VINSTR,DEST,SRCA,SRCB) _vbx(MOD_MASKED | MOD_NONE,VMODE,VINSTR,DEST,SRCA,SRCB)

#define vbx_setup_mask_(VINSTR,SRC)\
	do{ \
	typedef typeof(*(SRC)) src_t;\
	int is_signed=((src_t)-1 <0);\
	if( is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*signed word*/ \
		vbxasm_setup_mask(SVWS,VINSTR,(vbx_word_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*signed half*/ \
		vbxasm_setup_mask(SVHS,VINSTR,(vbx_half_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*signed byte*/ \
		vbxasm_setup_mask(SVBS,VINSTR,(vbx_byte_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*unsigned word*/ \
		vbxasm_setup_mask(SVWU,VINSTR,(vbx_uword_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*unsigned half*/ \
		vbxasm_setup_mask(SVHU,VINSTR,(vbx_uhalf_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*unsigned byte*/ \
		vbxasm_setup_mask(SVBU,VINSTR,(vbx_ubyte_t*)SRC); \
	}while(0)

#define vbx_setup_mask_masked_(VINSTR,SRC)\
	do{ \
	typedef typeof(*(SRC)) src_t;\
	int is_signed=((src_t)-1 <0);\
	if( is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*signed word*/ \
		vbxasm_setup_mask_masked(SVWS,VINSTR,(vbx_word_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*signed half*/ \
		vbxasm_setup_mask_masked(SVHS,VINSTR,(vbx_half_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*signed byte*/ \
		vbxasm_setup_mask_masked(SVBS,VINSTR,(vbx_byte_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*unsigned word*/ \
		vbxasm_setup_mask_masked(SVWU,VINSTR,(vbx_uword_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*unsigned half*/ \
		vbxasm_setup_mask_masked(SVHU,VINSTR,(vbx_uhalf_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*unsigned byte*/ \
		vbxasm_setup_mask_masked(SVBU,VINSTR,(vbx_ubyte_t*)SRC); \
	}while(0)

#ifndef __cplusplus

#define vbx_setup_mask(VINSTR,SRC) \
	vbx_setup_mask_(VINSTR,SRC)

#define vbx_setup_mask_masked(VINSTR,SRC) \
	 vbx_setup_mask_masked_(VINSTR,SRC)

#endif


#endif // __VBX_CPROTO_H
