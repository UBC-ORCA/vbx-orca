//VBXCOPYRIGHTTAG
#ifndef __VBX_CPROTO_H
#define __VBX_CPROTO_H

#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVBU VVBBBUUU
#define VVBUU VVBBBUUU
#define VVBBU VVBBBUUU
#define VVBBUU VVBBBUUU
#define VVHU VVHHHUUU
#define VVHUU VVHHHUUU
#define VVHHU VVHHHUUU
#define VVHHUU VVHHHUUU
#define VVWU VVWWWUUU
#define VVWUU VVWWWUUU
#define VVWWU VVWWWUUU
#define VVWWUU VVWWWUUU
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define VVB VVBBBSSS
#define VVBS VVBBBSSS
#define VVBSS VVBBBSSS
#define VVBB VVBBBSSS
#define VVBBS VVBBBSSS
#define VVBBSS VVBBBSSS
#define VVH VVHHHSSS
#define VVHS VVHHHSSS
#define VVHSS VVHHHSSS
#define VVHH VVHHHSSS
#define VVHHS VVHHHSSS
#define VVHHSS VVHHHSSS
#define VVW VVWWWSSS
#define VVWS VVWWWSSS
#define VVWSS VVWWWSSS
#define VVWW VVWWWSSS
#define VVWWS VVWWWSSS
#define VVWWSS VVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVBU SVBBBUUU
#define SVBUU SVBBBUUU
#define SVBBU SVBBBUUU
#define SVBBUU SVBBBUUU
#define SVHU SVHHHUUU
#define SVHUU SVHHHUUU
#define SVHHU SVHHHUUU
#define SVHHUU SVHHHUUU
#define SVWU SVWWWUUU
#define SVWUU SVWWWUUU
#define SVWWU SVWWWUUU
#define SVWWUU SVWWWUUU
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define SVB SVBBBSSS
#define SVBS SVBBBSSS
#define SVBSS SVBBBSSS
#define SVBB SVBBBSSS
#define SVBBS SVBBBSSS
#define SVBBSS SVBBBSSS
#define SVH SVHHHSSS
#define SVHS SVHHHSSS
#define SVHSS SVHHHSSS
#define SVHH SVHHHSSS
#define SVHHS SVHHHSSS
#define SVHHSS SVHHHSSS
#define SVW SVWWWSSS
#define SVWS SVWWWSSS
#define SVWSS SVWWWSSS
#define SVWW SVWWWSSS
#define SVWWS SVWWWSSS
#define SVWWSS SVWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSBU VSBBBUUU
#define VSBUU VSBBBUUU
#define VSBBU VSBBBUUU
#define VSBBUU VSBBBUUU
#define VSHU VSHHHUUU
#define VSHUU VSHHHUUU
#define VSHHU VSHHHUUU
#define VSHHUU VSHHHUUU
#define VSWU VSWWWUUU
#define VSWUU VSWWWUUU
#define VSWWU VSWWWUUU
#define VSWWUU VSWWWUUU
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VSB VSBBBSSS
#define VSBS VSBBBSSS
#define VSBSS VSBBBSSS
#define VSBB VSBBBSSS
#define VSBBS VSBBBSSS
#define VSBBSS VSBBBSSS
#define VSH VSHHHSSS
#define VSHS VSHHHSSS
#define VSHSS VSHHHSSS
#define VSHH VSHHHSSS
#define VSHHS VSHHHSSS
#define VSHHSS VSHHHSSS
#define VSW VSWWWSSS
#define VSWS VSWWWSSS
#define VSWSS VSWWWSSS
#define VSWW VSWWWSSS
#define VSWWS VSWWWSSS
#define VSWWSS VSWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEBU VEBBBUUU
#define VEBUU VEBBBUUU
#define VEBBU VEBBBUUU
#define VEBBUU VEBBBUUU
#define VEHU VEHHHUUU
#define VEHUU VEHHHUUU
#define VEHHU VEHHHUUU
#define VEHHUU VEHHHUUU
#define VEWU VEWWWUUU
#define VEWUU VEWWWUUU
#define VEWWU VEWWWUUU
#define VEWWUU VEWWWUUU
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define VEB VEBBBSSS
#define VEBS VEBBBSSS
#define VEBSS VEBBBSSS
#define VEBB VEBBBSSS
#define VEBBS VEBBBSSS
#define VEBBSS VEBBBSSS
#define VEH VEHHHSSS
#define VEHS VEHHHSSS
#define VEHSS VEHHHSSS
#define VEHH VEHHHSSS
#define VEHHS VEHHHSSS
#define VEHHSS VEHHHSSS
#define VEW VEWWWSSS
#define VEWS VEWWWSSS
#define VEWSS VEWWWSSS
#define VEWW VEWWWSSS
#define VEWWS VEWWWSSS
#define VEWWSS VEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEBU SEBBBUUU
#define SEBUU SEBBBUUU
#define SEBBU SEBBBUUU
#define SEBBUU SEBBBUUU
#define SEHU SEHHHUUU
#define SEHUU SEHHHUUU
#define SEHHU SEHHHUUU
#define SEHHUU SEHHHUUU
#define SEWU SEWWWUUU
#define SEWUU SEWWWUUU
#define SEWWU SEWWWUUU
#define SEWWUU SEWWWUUU
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS
#define SEB SEBBBSSS
#define SEBS SEBBBSSS
#define SEBSS SEBBBSSS
#define SEBB SEBBBSSS
#define SEBBS SEBBBSSS
#define SEBBSS SEBBBSSS
#define SEH SEHHHSSS
#define SEHS SEHHHSSS
#define SEHSS SEHHHSSS
#define SEHH SEHHHSSS
#define SEHHS SEHHHSSS
#define SEHHSS SEHHHSSS
#define SEW SEWWWSSS
#define SEWS SEWWWSSS
#define SEWSS SEWWWSSS
#define SEWW SEWWWSSS
#define SEWWS SEWWWSSS
#define SEWWSS SEWWWSSS

__attribute__((always_inline)) static inline void vbx_VVWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWWWSSS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWWWSSS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWWWSSS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWWWSSS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWWWSSS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWWWSSS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWWWSSS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWWWSSS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWWWSSS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWWWSSS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWWWSSS, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWWWSSS, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWWWSSS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWWWSSS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWWWSSS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWWWSSS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWWWSSS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWWWSSS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWWWSSS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWWWSSS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWWWSSS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWWWSSS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWWWSSS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWWWSSS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWWWSSS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWWWSSS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWWWSSS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWWWSSS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWWWSSS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWWWSSS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWWWSSS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWWWSSS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWWWSSS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWWWSSS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWWWSSS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWWWSSS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWWWSSS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWWWSSS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWWWSSS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VVWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VVWWWUUU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, VVWWWUUU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, VVWWWUUU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, VVWWWUUU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, VVWWWUUU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, VVWWWUUU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, VVWWWUUU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, VVWWWUUU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, VVWWWUUU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, VVWWWUUU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, VVWWWUUU, VSLT, v_out, v_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, VVWWWUUU, VSGT, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, VVWWWUUU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, VVWWWUUU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, VVWWWUUU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, VVWWWUUU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, VVWWWUUU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, VVWWWUUU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VVWWWUUU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VVWWWUUU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VVWWWUUU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VVWWWUUU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, VVWWWUUU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VVWWWUUU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VVWWWUUU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VVWWWUUU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VVWWWUUU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VVWWWUUU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VVWWWUUU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VVWWWUUU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VVWWWUUU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VVWWWUUU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VVWWWUUU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VVWWWUUU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VVWWWUUU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VVWWWUUU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VVWWWUUU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VVWWWUUU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VVWWWUUU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VVWWWUUU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWWWSSS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWWWSSS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWWWSSS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWWWSSS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWWWSSS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWWWSSS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWWWSSS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWWWSSS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWWWSSS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWWWSSS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWWWSSS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWWWSSS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWWWSSS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWWWSSS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWWWSSS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWWWSSS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWWWSSS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SVWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWUUU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWUUU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWUUU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWUUU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm(modify, SVWWWUUU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWUUU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWUUU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWUUU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWUUU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWUUU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWUUU, VSLT, v_out, s_in1, v_in2 );
		break;
	case VSGT:
		vbxasm(modify, SVWWWUUU, VSGT, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm(modify, SVWWWUUU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm(modify, SVWWWUUU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWUUU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWUUU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm(modify, SVWWWUUU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VMOV:
		vbxasm(modify, SVWWWUUU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWUUU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWUUU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWUUU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWUUU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWUUU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWUUU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SVWWWUUU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SVWWWUUU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SVWWWUUU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SVWWWUUU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SVWWWUUU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SVWWWUUU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SVWWWUUU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SVWWWUUU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SVWWWUUU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SVWWWUUU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SVWWWUUU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SVWWWUUU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SVWWWUUU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SVWWWUUU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SVWWWUUU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SVWWWUUU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWSSS, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWSSS, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWWWSSS, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWSSS,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWSSS, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWSSS, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWSSS, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWSSS, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWSSS,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWWWSSS,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWWWSSS, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWWWSSS, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWSSS, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWSSS,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWWWSSS,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWWWSSS,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWWWSSS, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWSSS, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWSSS, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWSSS, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWSSS, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWSSS, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWSSS, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWWWSSS cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VSWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t s_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SVWWWUUU, VADD, v_out, s_in2, v_in1 );
		break;
	case VSUB:
		vbxasm(modify, SVWWWUUU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDFXP:
		vbxasm(modify, SVWWWUUU, VADDFXP, v_out, s_in2, v_in1 );
		break;
	case VSUBFXP:
		vbxasm(modify, SVWWWUUU,VADD, v_out, -s_in2, v_in1);
		break;
	case VADDC:
		vbxasm(modify, SVWWWUUU, VADDC, v_out, s_in2, v_in1 );
		break;
	case VSUBB:
		vbxasm(modify, SVWWWUUU,VADD, v_out, -s_in2, v_in1);
		break;
	case VABSDIFF:
		vbxasm(modify, SVWWWUUU, VABSDIFF, v_out, s_in2, v_in1 );
		break;
	case VMUL:
		vbxasm(modify, SVWWWUUU, VMUL, v_out, s_in2, v_in1 );
		break;
	case VMULHI:
		vbxasm(modify, SVWWWUUU, VMULHI, v_out, s_in2, v_in1 );
		break;
	case VMULFXP:
		vbxasm(modify, SVWWWUUU, VMULFXP, v_out, s_in2, v_in1 );
		break;
	case VSLT:
		vbxasm(modify, SVWWWUUU,VSGT, v_out, s_in2, v_in1);
		break;
	case VSGT:
		vbxasm(modify, SVWWWUUU,VSLT, v_out, s_in2, v_in1);
		break;
	case VAND:
		vbxasm(modify, SVWWWUUU, VAND, v_out, s_in2, v_in1 );
		break;
	case VOR:
		vbxasm(modify, SVWWWUUU, VOR, v_out, s_in2, v_in1 );
		break;
	case VXOR:
		vbxasm(modify, SVWWWUUU, VXOR, v_out, s_in2, v_in1 );
		break;
	case VSHL:
		vbxasm(modify, SVWWWUUU,VMUL, v_out, (1<<s_in2), v_in1);
		break;
	case VSHR:
		if(s_in2){vbxasm( modify, SVWWWUUU,VMULHI,v_out,(1<<((max(sizeof(*v_out),sizeof(*v_in1))*8)-(s_in2))),v_in1);}else {vbxasm(modify, SVWWWUUU,VADD, v_out, 0, v_in1);};
		break;
	case VMOV:
		vbxasm(modify, SVWWWUUU, VMOV, v_out, s_in2, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SVWWWUUU, VCMV_LEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SVWWWUUU, VCMV_GTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SVWWWUUU, VCMV_LTZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SVWWWUUU, VCMV_GEZ, v_out, s_in2, v_in1 );
		break;
	case VCMV_Z:
		vbxasm(modify, SVWWWUUU, VCMV_Z, v_out, s_in2, v_in1 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SVWWWUUU, VCMV_NZ, v_out, s_in2, v_in1 );
		break;
	case VCUSTOM0:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM1:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM2:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM3:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM4:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM5:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM6:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM7:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM8:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM9:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM10:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM11:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM12:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM13:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM14:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	case VCUSTOM15:
		assert(0&&"modify, SVWWWUUU cannot use psuedo-op specifier ");;
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWWWSSS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWWWSSS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWWWSSS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWWWSSS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWWWSSS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWWWSSS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWWWSSS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWWWSSS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWWWSSS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWWWSSS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWWWSSS, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWWWSSS, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWWWSSS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWWWSSS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWWWSSS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWWWSSS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWWWSSS, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWWWSSS, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWWWSSS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWWWSSS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWWWSSS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWWWSSS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWWWSSS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWWWSSS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWWWSSS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWWWSSS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWWWSSS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWWWSSS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWWWSSS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWWWSSS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWWWSSS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWWWSSS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWWWSSS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWWWSSS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWWWSSS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWWWSSS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWWWSSS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWWWSSS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWWWSSS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWWWSSS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_VEWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, VEWWWUUU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, VEWWWUUU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, VEWWWUUU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, VEWWWUUU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, VEWWWUUU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, VEWWWUUU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, VEWWWUUU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, VEWWWUUU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, VEWWWUUU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, VEWWWUUU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, VEWWWUUU, VSLT, v_out, v_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, VEWWWUUU, VSGT, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, VEWWWUUU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, VEWWWUUU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, VEWWWUUU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, VEWWWUUU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, VEWWWUUU, VSHR, v_out, v_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, VEWWWUUU, VMOV, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, VEWWWUUU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, VEWWWUUU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, VEWWWUUU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, VEWWWUUU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, VEWWWUUU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, VEWWWUUU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, VEWWWUUU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, VEWWWUUU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, VEWWWUUU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, VEWWWUUU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, VEWWWUUU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, VEWWWUUU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, VEWWWUUU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, VEWWWUUU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, VEWWWUUU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, VEWWWUUU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, VEWWWUUU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, VEWWWUUU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, VEWWWUUU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, VEWWWUUU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, VEWWWUUU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, VEWWWUUU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWWWSSS(int modify, vinstr_t v_op, vbx_word_t *v_out, vbx_word_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWWWSSS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWWWSSS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWWWSSS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWWWSSS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWWWSSS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWWWSSS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWWWSSS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWWWSSS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWWWSSS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWWWSSS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWWWSSS, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWWWSSS, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWWWSSS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWWWSSS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWWWSSS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWWWSSS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWWWSSS, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWWWSSS, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWWWSSS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWWWSSS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWWWSSS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWWWSSS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWWWSSS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWWWSSS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWWWSSS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWWWSSS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWWWSSS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWWWSSS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWWWSSS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWWWSSS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWWWSSS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWWWSSS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWWWSSS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWWWSSS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWWWSSS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWWWSSS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWWWSSS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWWWSSS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWWWSSS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWWWSSS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbx_SEWWWUUU(int modify, vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t  s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm(modify, SEWWWUUU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm(modify, SEWWWUUU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm(modify, SEWWWUUU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm(modify, SEWWWUUU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm(modify, SEWWWUUU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm(modify, SEWWWUUU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm(modify, SEWWWUUU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm(modify, SEWWWUUU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm(modify, SEWWWUUU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm(modify, SEWWWUUU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VSLT:
		vbxasm(modify, SEWWWUUU, VSLT, v_out, s_in1, 0 );
		break;
	case VSGT:
		vbxasm(modify, SEWWWUUU, VSGT, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm(modify, SEWWWUUU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm(modify, SEWWWUUU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm(modify, SEWWWUUU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm(modify, SEWWWUUU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm(modify, SEWWWUUU, VSHR, v_out, s_in1, 0 );
		break;
	case VMOV:
		vbxasm(modify, SEWWWUUU, VMOV, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm(modify, SEWWWUUU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm(modify, SEWWWUUU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm(modify, SEWWWUUU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm(modify, SEWWWUUU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm(modify, SEWWWUUU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm(modify, SEWWWUUU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm(modify, SEWWWUUU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm(modify, SEWWWUUU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm(modify, SEWWWUUU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm(modify, SEWWWUUU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm(modify, SEWWWUUU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm(modify, SEWWWUUU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm(modify, SEWWWUUU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm(modify, SEWWWUUU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm(modify, SEWWWUUU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm(modify, SEWWWUUU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm(modify, SEWWWUUU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm(modify, SEWWWUUU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm(modify, SEWWWUUU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm(modify, SEWWWUUU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm(modify, SEWWWUUU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm(modify, SEWWWUUU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}
//


#define _vbx(MODIFY,VMODE,VINSTR,DEST,SRCA,SRCB)  vbx_##VMODE(MODIFY,VINSTR,DEST,SRCA,SRCB)

#define vbx(VMODE,VINSTR,DEST,SRCA,SRCB)            _vbx( MOD_NONE, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked(VMODE,VINSTR,DEST,SRCA,SRCB)     _vbx( MOD_MASKED, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_acc(VMODE,VINSTR,DEST,SRCA,SRCB)        _vbx(MOD_ACC, VMODE,VINSTR,DEST,SRCA,SRCB)
#define vbx_masked_acc(VMODE,VINSTR,DEST,SRCA,SRCB) _vbx(MOD_MASKED | MOD_NONE,VMODE,VINSTR,DEST,SRCA,SRCB)

#define vbx_setup_mask_(VINSTR,SRC)\
	do{ \
	typedef typeof(*(SRC)) src_t;\
	int is_signed=((src_t)-1 <0);\
	if( is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*signed word*/ \
		vbxasm_setup_mask(SVWS,VINSTR,(vbx_word_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*signed half*/ \
		vbxasm_setup_mask(SVHS,VINSTR,(vbx_half_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*signed byte*/ \
		vbxasm_setup_mask(SVBS,VINSTR,(vbx_byte_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*unsigned word*/ \
		vbxasm_setup_mask(SVWU,VINSTR,(vbx_uword_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*unsigned half*/ \
		vbxasm_setup_mask(SVHU,VINSTR,(vbx_uhalf_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*unsigned byte*/ \
		vbxasm_setup_mask(SVBU,VINSTR,(vbx_ubyte_t*)SRC); \
	}while(0)

#define vbx_setup_mask_masked_(VINSTR,SRC)\
	do{ \
	typedef typeof(*(SRC)) src_t;\
	int is_signed=((src_t)-1 <0);\
	if( is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*signed word*/ \
		vbxasm_setup_mask_masked(SVWS,VINSTR,(vbx_word_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*signed half*/ \
		vbxasm_setup_mask_masked(SVHS,VINSTR,(vbx_half_t*)SRC); \
	if( is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*signed byte*/ \
		vbxasm_setup_mask_masked(SVBS,VINSTR,(vbx_byte_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_word_t)) /*unsigned word*/ \
		vbxasm_setup_mask_masked(SVWU,VINSTR,(vbx_uword_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_half_t)) /*unsigned half*/ \
		vbxasm_setup_mask_masked(SVHU,VINSTR,(vbx_uhalf_t*)SRC); \
	if( !is_signed && sizeof(src_t)==sizeof(vbx_byte_t)) /*unsigned byte*/ \
		vbxasm_setup_mask_masked(SVBU,VINSTR,(vbx_ubyte_t*)SRC); \
	}while(0)

#ifndef __cplusplus

#define vbx_setup_mask(VINSTR,SRC) \
	vbx_setup_mask_(VINSTR,SRC)

#define vbx_setup_mask_masked(VINSTR,SRC) \
	 vbx_setup_mask_masked_(VINSTR,SRC)

#endif


#endif // __VBX_CPROTO_H
